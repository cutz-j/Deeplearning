{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import os\n",
    "import pickle as pickle\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    val_len = int(len(MNIST[\"data\"]) * VAL_RATIO)\n",
    "    test_len = int(len(MNIST[\"data\"]) * TEST_RATIO)\n",
    "    train_len = int(len(MNIST[\"data\"]) - val_len - test_len)\n",
    "    X_train= MNIST[\"data\"][:train_len].reshape((-1,1,28,28)) / 255.0\n",
    "    X_val = MNIST[\"data\"][train_len : train_len + val_len].reshape((-1,1,28,28)) / 255.0\n",
    "    X_test = MNIST[\"data\"][-test_len:].reshape((-1,1,28,28)) / 255.0\n",
    "    y_train = MNIST[\"target\"][:train_len]\n",
    "    y_val = MNIST[\"target\"][train_len : train_len + val_len] \n",
    "    y_test = MNIST[\"target\"][-test_len:]\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully connected \n",
    "def affine_forward(x, w, b):\n",
    "    #print(\"affine forward\")\n",
    "    #print(\"affine X shape : {}, w shape {}, bias shape {}\".format(x.shape, w.shape, b.shape))\n",
    "    out = None\n",
    "    x_row = x.reshape(x.shape[0], -1)\n",
    "    \n",
    "    out = np.matmul(x_row,w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    #print(\"affine_backward\")\n",
    "    #print(\"dout : {}\".format(dout.shape))\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    dx = np.matmul(dout, w.T)\n",
    "    dx = dx.reshape(x.shape)\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    dw = np.matmul(x.T, dout)\n",
    "    db = np.sum(dout, axis = 0)\n",
    "    return dx, dw, db\n",
    "\n",
    "#relu\n",
    "def relu_forward(x):\n",
    "   # print(\"relu forward\")\n",
    "    out = None\n",
    "    out = np.maximum(x,0)\n",
    "\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    #print(\"relu backward\")\n",
    "    dx, x = None, cache\n",
    "    #print(\"dout : {}, x : {}\".format(dout.shape, x.shape))\n",
    "    dx = (x > 0) * dout\n",
    "    return dx\n",
    "\n",
    "#dropout\n",
    "def dropout_forward(x, dropout_param):\n",
    "    p, mode = dropout_param[\"p\"], dropout_param[\"mode\"]\n",
    "    if \"seed\" in dropout_param:\n",
    "        np.random.seed(dropout_param[\"seed\"])\n",
    "\n",
    "    mask = None\n",
    "    out = None\n",
    "\n",
    "    if mode == \"train\":\n",
    "        mask = np.random.binomial(np.ones_like(x), p) / p\n",
    "    elif mode == \"test\":\n",
    "        out = x\n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param[\"mode\"]\n",
    "\n",
    "    dx = None\n",
    "    if mode == \"train\":\n",
    "        dx = dout * mask\n",
    "    elif mode == \"test\":\n",
    "        dx = dout\n",
    "    return dx\n",
    "\n",
    "#convolution \n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    #print(\"conv forward\")\n",
    "    out = None\n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    padding, stride = conv_param[\"pad\"], conv_param[\"stride\"]\n",
    "    \n",
    "    assert (H + 2*padding - HH) % stride == 0, \"shape error\"\n",
    "    assert (W + 2*padding - WW) % stride == 0, \"shape error\"\n",
    "    \n",
    "    out_N = N\n",
    "    out_C = F\n",
    "    out_H = int((H + 2*padding - HH) / stride + 1)\n",
    "    out_W = int((W + 2*padding - WW) / stride + 1)\n",
    "    out = np.zeros((out_N, out_C, out_H, out_W))\n",
    "    padded_x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))\n",
    "       \n",
    "    for n in range(out_N):\n",
    "        for c in range(out_C):\n",
    "            for h in range(out_H):\n",
    "                for w_ in range(out_W):\n",
    "                    out[n,c,h,w_] = np.sum(padded_x[n,:, h*stride: h*stride + HH, w_*stride : w_*stride + WW] * w[c]) + b[c]\n",
    "    \n",
    "    cache = (x, w, b, conv_param)\n",
    "    #print(\"conv : x {} w {} b {}\".format(x.shape, w.shape, b.shape))\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "    #print(\"conv backward\")\n",
    "    dx, dw, db = None, None, None\n",
    "    x, w, b, conv_param = cache\n",
    "    padding = conv_param[\"pad\"]\n",
    "    stride = conv_param[\"stride\"]\n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    \n",
    "    assert (H + 2*padding - HH) % stride == 0, \"shape error\"\n",
    "    assert (W + 2*padding - WW) % stride == 0, \"shape error\"\n",
    "\n",
    "    dx = np.zeros_like(x)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = np.zeros_like(b)\n",
    "    \n",
    "    out_N = N\n",
    "    out_C = F\n",
    "    out_H = int((H + 2*padding - HH) / stride + 1)\n",
    "    out_W = int((W + 2*padding - WW) / stride + 1)\n",
    "    out = np.zeros((out_N, out_C, out_H, out_W))\n",
    "    padded_x = np.pad(x, ((0,0), (0,0), (padding, padding), (padding, padding)))\n",
    "    padded_dx = np.pad(dx, ((0,0), (0,0), (padding, padding), (padding, padding)))\n",
    "    #print(\"padded dx : {}  dout : {}  w : {}\".format(padded_dx.shape, dout.shape, w.shape))\n",
    "    #backpropagation \n",
    "    for n in range(out_N):\n",
    "        for c in range(out_C):\n",
    "            for h in range(out_H):\n",
    "                for w_ in range(out_W):\n",
    "                    window_x = padded_x[n, :, h*stride:h*stride + HH, w_*stride:w_*stride + WW]\n",
    "                    #bias 는 add gate\n",
    "                    db[c] += dout[n,c,h,w_]\n",
    "                    #weight는 mul gate\n",
    "                    dw[c] += window_x * dout[n,c,h,w_]\n",
    "                    #x는 mul gate\n",
    "                    padded_dx[n, :, h*stride:h*stride + HH, w_*stride:w_*stride + WW] += w[c]* dout[n,c,h,w_]\n",
    "    #padding한 만큼 자르기\n",
    "    dx = padded_dx[:, :, padding:padding + H, padding : padding + W]\n",
    "    \n",
    "    return dx, dw, db\n",
    "\n",
    "#max pooling\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "    #print(\"pooling forward\")\n",
    "    out = None\n",
    "    N, C, H, W = x.shape\n",
    "    pooling_h = pool_param[\"pool_height\"]\n",
    "    pooling_w = pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "    \n",
    "    out_n = N\n",
    "    out_c = C\n",
    "    out_h = int((H - pooling_h) / stride + 1)\n",
    "    out_w = int((W - pooling_w) / stride + 1)\n",
    "    \n",
    "    out = np.zeros((out_n,out_c, out_h, out_w))\n",
    "    for n in range(out_n):\n",
    "        for c in range(out_c):\n",
    "            for h in range(out_h):\n",
    "                for w in range(out_w):\n",
    "                    window_x = x[n,c,h*stride : h*stride + pooling_h, w*stride : w*stride + pooling_w]\n",
    "                    out[n,c,h,w] = np.max(window_x)\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "    #print(\"pooling backward\")\n",
    "    dx = None\n",
    "    x, pool_param = cache\n",
    "    N, C, H, W = x.shape\n",
    "    pooling_h = pool_param[\"pool_height\"]\n",
    "    pooling_w = pool_param[\"pool_width\"]\n",
    "    stride = pool_param[\"stride\"]\n",
    "    \n",
    "    out_n = N\n",
    "    out_c = C\n",
    "    out_h = int((H - pooling_h) / stride + 1)\n",
    "    out_w = int((W - pooling_w) / stride + 1)\n",
    "    #print(\"dout : {}\".format(dout.shape))\n",
    "    dx = np.zeros_like(x)\n",
    "    for n in range(out_n):\n",
    "        for c in range(out_c):\n",
    "            for h in range(out_h):\n",
    "                for w in range(out_w):\n",
    "                    window_x = x[n,c,h*stride : h*stride + pooling_h, w*stride : w*stride + pooling_w]\n",
    "                    mask = (window_x == np.max(window_x))\n",
    "                    dx[n,c,h*stride : h*stride + pooling_h,w*stride : w*stride + pooling_w] = mask * dout[n,c,h,w]\n",
    "\n",
    "    return dx\n",
    "#softmax\n",
    "def softmax(x,y):\n",
    "    #print(\"softmax\")\n",
    "    logits = x - np.max(x, axis= 1, keepdims = True)\n",
    "    Z = np.sum(np.exp(logits), axis =1 ,keepdims = True)\n",
    "    log_probs = logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    y = y.astype(np.int8)\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N) , y] -=1\n",
    "    dx /= N\n",
    "    return loss, dx\n",
    "\n",
    "#For predict\n",
    "def predict_softmax(x):\n",
    "    #print(\"predict softmax\")\n",
    "    logits = x - np.max(x, axis= 1, keepdims = True)\n",
    "    Z = np.sum(np.exp(logits), axis =1 ,keepdims = True)\n",
    "    log_probs = logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1)\n",
    "\n",
    "    w -= config[\"learning_rate\"] * dw\n",
    "    return w, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model sequences    \n",
    "conv_param = {\"pad\", \"stride\", \"num_filters\", \"channel\", \"filter_size\"}    \n",
    "pool_param = {\"pool_height\", \"pool_width\", \"stride\"}\n",
    "dense_param = {\"num_filters\", \"H\", \"W\", \"hidden_dim\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class Sequential():\n",
    "    def __init__(self, weight_scale = 1e-03, reg = 0.0, dtype = np.float32):\n",
    "        self.pipeline = {}\n",
    "        self.weight_bias = []\n",
    "        self.layers_name = set([\"Relu\", \"Conv\", \"Dense\", \"Pooling\"])\n",
    "        self.weight_scale = weight_scale\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        self.num_conv = 1\n",
    "        self.num_bias = 1\n",
    "        self.num_relu = 1\n",
    "        self.num_pool = 1\n",
    "        self.num_dense = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add(self, layer, params = None):\n",
    "        \n",
    "        assert layer in self.layers_name, \"You have to choose layer in {}\".format(self.layers_name)\n",
    "            \n",
    "        if layer == \"Conv\":\n",
    "            assert params is not None, \"Enter params\"\n",
    "            self.pipeline[\"Conv_{}\".format(self.num_conv)] = [self.weight_scale * np.random.randn(params[\"num_filters\"], params[\"channel\"], params[\"filter_size\"], \n",
    "                                                                            params[\"filter_size\"]),np.zeros(params[\"num_filters\"]), params]\n",
    "            self.weight_bias.append(\"Conv_{}\".format(self.num_conv))\n",
    "            self.weight_bias.append(\"Bias_{}\".format(self.num_bias))\n",
    "            self.num_conv += 1\n",
    "            self.num_bias += 1\n",
    "        \n",
    "        elif layer == \"Relu\":\n",
    "            self.pipeline[\"Relu_{}\".format(self.num_relu)] = (None, None)\n",
    "            self.num_relu +=1\n",
    "            \n",
    "        elif layer == \"Dense\":\n",
    "            assert params is not None, \"Enter params\"\n",
    "            self.pipeline[\"Dense_{}\".format(self.num_dense)] = [self.weight_scale * np.random.randn(params[\"num_filters\"] * params[\"H\"] * params[\"W\"] , params[\"hidden_dim\"]), \n",
    "                                                                np.zeros(params[\"hidden_dim\"]), params]\n",
    "            self.weight_bias.append(\"Dense_{}\".format(self.num_dense))\n",
    "            self.weight_bias.append(\"Bias_{}\".format(self.num_bias))\n",
    "            self.num_dense += 1\n",
    "            self.num_bias += 1\n",
    "        \n",
    "        elif layer == \"Pooling\":\n",
    "            assert params is not None, \"Enter params\"\n",
    "            self.pipeline[\"Pooling_{}\".format(self.num_pool)] = (None, params)\n",
    "            self.num_pool += 1\n",
    "            \n",
    "    def type_fix(self):\n",
    "        for layer, weight in self.pipeline.items():\n",
    "            if layer[:4] == \"Pool\" or layer[:4] == \"Relu\":\n",
    "                continue\n",
    "            self.pipeline[layer] = weight.astype(self.dtype)\n",
    "            \n",
    "    def loss(self, x, y= None):\n",
    "        cache = None\n",
    "        #For backpropagation.\n",
    "        cache_list = []\n",
    "        weight_list = []\n",
    "        bias_list = []\n",
    "        for layer in self.pipeline.keys():\n",
    "            layer_name = layer[:4]\n",
    "            if layer_name == \"Pool\":\n",
    "                pool_param = self.pipeline[layer][1]\n",
    "                x, cache = max_pool_forward_naive(x, pool_param)\n",
    "                cache_list.append(cache)\n",
    "                \n",
    "            elif layer_name == \"Relu\":\n",
    "                x, cache = relu_forward(x)\n",
    "                cache_list.append(cache)\n",
    "                \n",
    "            elif layer_name == \"Conv\":\n",
    "                filter_size = self.pipeline[layer][0].shape[2]\n",
    "                conv_weight = self.pipeline[layer][0]\n",
    "                bias_weight= self.pipeline[layer][1]\n",
    "                weight_list.append(conv_weight)\n",
    "                bias_list.append(bias_weight)\n",
    "                conv_param = self.pipeline[layer][2]\n",
    "                x, cache = conv_forward_naive(x, conv_weight, bias_weight, conv_param)\n",
    "                cache_list.append(cache)\n",
    "                \n",
    "            elif layer_name == \"Dens\":\n",
    "                dense_weight = self.pipeline[layer][0]\n",
    "                bias_weight = self.pipeline[layer][1]\n",
    "                x, cache = affine_forward(x, dense_weight, bias_weight)\n",
    "                weight_list.append(dense_weight)\n",
    "                bias_list.append(bias_weight)\n",
    "                cache_list.append(cache)\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                assert 1 != 1, \"Improper layer is in pipeline!!\"\n",
    "                \n",
    "        #backpropagation, x: score(probability)\n",
    "        b_weight_bias_list = []\n",
    "        weight_idx = 1\n",
    "        b_cache_idx = 1\n",
    "        b_layer_idx = 1\n",
    "        data_loss, dx = softmax(x, y)\n",
    "        \n",
    "        while b_cache_idx <= len(cache_list):\n",
    "            layer_name = list(self.pipeline.keys())[-b_layer_idx][:4]\n",
    "            if layer_name == \"Conv\":\n",
    "                dx, dW, db = conv_backward_naive(dx, cache_list[-b_cache_idx])\n",
    "                b_cache_idx += 1\n",
    "                dW += self.reg * weight_list[-weight_idx]\n",
    "                weight_idx += 1\n",
    "                b_weight_bias_list.append(db)\n",
    "                b_weight_bias_list.append(dW)\n",
    "                \n",
    "                \n",
    "                \n",
    "            elif layer_name == \"Pool\":\n",
    "                dx = max_pool_backward_naive(dx, cache_list[-b_cache_idx])\n",
    "                b_cache_idx += 1\n",
    "                \n",
    "            elif layer_name == \"Relu\":\n",
    "                dx = relu_backward(dx, cache_list[-b_cache_idx])\n",
    "                b_cache_idx += 1\n",
    "                \n",
    "            elif layer_name == \"Dens\":\n",
    "                dx, dW, db = affine_backward(dx, cache_list[-b_cache_idx])\n",
    "                b_cache_idx += 1\n",
    "                dW += self.reg * weight_list[-weight_idx]\n",
    "                weight_idx += 1\n",
    "                b_weight_bias_list.append(db)\n",
    "                b_weight_bias_list.append(dW)\n",
    "                \n",
    "                \n",
    "            b_layer_idx +=  1\n",
    "            \n",
    "        reg_loss = 0.5 * self.reg * sum(np.sum(W * W) for W in weight_list)\n",
    "        \n",
    "        \n",
    "        loss = data_loss + reg_loss\n",
    "        grads = {key: value for key,value in zip(self.weight_bias,b_weight_bias_list[::-1])}\n",
    "                \n",
    "        return loss, grads   \n",
    "    \n",
    "    def compile_(self, optimizer, num_epochs = 10, **kwargs):\n",
    "        self.batch_size = kwargs.pop(\"batch_size\", 256)\n",
    "        self.loss_history = []\n",
    "        self.update_rule = optimizer\n",
    "        self.optimizer_configs = {\"learning_rate\" : 1e-02}\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.checkpoint_name = kwargs.pop(\"checkpoint_name\" , None)\n",
    "        self.verbose = kwargs.pop(\"verbose\", True)\n",
    "        \n",
    "        assert len(kwargs) <=0, \"Unrecognized arguments {}\".format(\", \".join(\"{}\".format(k) for k in list(kwargs.keys())))\n",
    "        \n",
    "        for p in self.pipeline:\n",
    "            d = {k:v for k,v in self.optimizer_configs.items()}\n",
    "            self.optimizer_configs[p] = d\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, X_data, y_data,batch_size = 256,num_epochs = 10):\n",
    "        num_train = X_data.shape[0]\n",
    "        num_batches = num_train // batch_size\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"epoch : {}\".format(epoch))\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            for i in range(num_batches):\n",
    "                X_batch = X_data[start:end]\n",
    "                y_batch = y_data[start:end]\n",
    "                num_bias = 1\n",
    "                loss, grads = self.loss(X_batch, y_batch)\n",
    "                #weight update\n",
    "                for layer,weights in self.pipeline.items():\n",
    "                    if layer[:4] == \"Pool\" or layer[:4] == \"Relu\":\n",
    "                        continue\n",
    "                    dw = grads[layer]\n",
    "                    config = self.optimizer_configs[layer]\n",
    "                    #print(\"{} before update : {}\".format(layer, weights[0]))\n",
    "                    next_w , next_config = self.update_rule(weights[0], dw,config)\n",
    "                    self.pipeline[layer][0] = next_w\n",
    "                    #print(\"after update : {}\".format(self.pipeline[layer][0]))\n",
    "                    self.optimizer_configs[layer] = next_config\n",
    "                    bias_layer = \"Bias_{}\".format(num_bias)\n",
    "                    #print(\"{} before update : {}\".format(bias_layer, weights[1]))\n",
    "                    dw = grads[bias_layer]\n",
    "                    next_w, next_config = self.update_rule(weights[1], dw, config)\n",
    "                    self.pipeline[layer][1] = next_w\n",
    "                    #print(\"after update : {}\".format(self.pipeline[layer][1]))\n",
    "                    self.optimizer_configs[bias_layer] = next_config\n",
    "                    num_bias +=1\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "\n",
    "\n",
    "    def predict(self,X_data, y_data,batch_size = 256):\n",
    "        N = X_data.shape[0]\n",
    "        num_batches = N//batch_size\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            print(i)\n",
    "            x = X_data[start:end]            \n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "            cache = None\n",
    "            for layer in self.pipeline.keys():\n",
    "                layer_name = layer[:4]\n",
    "                if layer_name == \"Pool\":\n",
    "                    pool_param = self.pipeline[layer][1]\n",
    "                    x, cache = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "                elif layer_name == \"Relu\":\n",
    "                    x, cache = relu_forward(x)\n",
    "\n",
    "                elif layer_name == \"Conv\":\n",
    "                    filter_size = self.pipeline[layer][0].shape[2]\n",
    "                    conv_weight = self.pipeline[layer][0]\n",
    "                    bias_weight= self.pipeline[layer][1]\n",
    "                    conv_param = self.pipeline[layer][2]\n",
    "                    x, cache = conv_forward_naive(x, conv_weight, bias_weight, conv_param)\n",
    "\n",
    "                elif layer_name == \"Dens\":\n",
    "                    dense_weight = self.pipeline[layer][0]\n",
    "                    bias_weight = self.pipeline[layer][1]\n",
    "                    x, cache = affine_forward(x, dense_weight, bias_weight)\n",
    "\n",
    "                else:\n",
    "                    assert 1 != 1, \"Improper layer is in pipeline!!\"\n",
    "        return predict_softmax(x)\n",
    "\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\"Conv\", params = {\"pad\" : 1, \"stride\" : 1,\"channel\" : 1, \"num_filters\" : 16,  \"filter_size\" : 3})\n",
    "model.add(\"Relu\")\n",
    "model.add(\"Pooling\", params = {\"pool_height\" : 2, \"pool_width\" : 2, \"stride\": 2})\n",
    "model.add(\"Conv\", params = {\"pad\" : 1, \"stride\" : 1,  \"channel\" : 16, \"num_filters\" : 32,  \"filter_size\" : 3})\n",
    "model.add(\"Relu\")\n",
    "model.add(\"Pooling\",  params = {\"pool_height\" : 2, \"pool_width\" : 2, \"stride\": 2})\n",
    "model.add(\"Conv\", params = {\"pad\" : 1, \"stride\" : 1,  \"channel\" : 32, \"num_filters\" : 64,  \"filter_size\" : 3})\n",
    "model.add(\"Relu\")\n",
    "model.add(\"Dense\", params = {\"num_filters\" : 64, \"H\": 7, \"W\": 7, \"hidden_dim\" : 10})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0:1],y_train[0:1], batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile_(optimizer = sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_train[0:1], y_train[0:1],batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlawjdghek",
   "language": "python",
   "name": "rlawjdghek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
