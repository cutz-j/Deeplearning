{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear & Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, input_dim, output_dim, name = 'linear'):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.weight = np.random.randn(input_dim, output_dim)\n",
    "        self.weight_gradient = np.zeros_like(self.weight)\n",
    "        self.b = np.random.randn(output_dim)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "        self.v_w, self.v_b = 0, 0\n",
    "        self.m, self.v = 0, 0\n",
    "        \n",
    "        self.x = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = np.dot(x, self.weight) + self.b\n",
    "        return res \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # batch  x feature_out dot feature_out x feature_in  >>> batch x feature_in\n",
    "        dx = np.dot(dout, self.weight.T)\n",
    "        \n",
    "        # feature_in x batch dot batch x feature_out >>> feature_in x feature_out (weight 매트릭스와 같은 모양)\n",
    "        self.weight_gradient = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "class ReLU(object):\n",
    "    def __init__(self, name='relu'):\n",
    "        self.name = name\n",
    "        self.mask = None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = x < 0\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.target = None\n",
    "        self.pred_prob = None\n",
    "        self.batch_size = None\n",
    "        \n",
    "    def __call__(self, pred, y):\n",
    "        return self.forward(pred, y)\n",
    "    \n",
    "    def softmax(self, pred):\n",
    "        # avoid overflow\n",
    "        z = pred - np.max(pred, axis=1).reshape(-1, 1)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=1).reshape(-1, 1)\n",
    "        softmax = numerator/denominator\n",
    "        \n",
    "        return softmax\n",
    "    \n",
    "    # batch size는 batchsize로 나눠서 한 값만 내보낸다.\n",
    "    def forward(self, pred, y):\n",
    "        self.target = y\n",
    "        self.batch_size = len(y)\n",
    "        \n",
    "        res =[]\n",
    "        # pred >>> 5, 10\n",
    "        self.pred_prob = self.softmax(pred)\n",
    "        predicted = self.pred_prob[np.arange(self.batch_size), y]\n",
    "        loss = -np.sum(np.log(predicted + 1e-7)) / self.batch_size\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):        \n",
    "        dx = self.pred_prob.copy()\n",
    "        dx[np.arange(self.batch_size), self.target] -= 1\n",
    "        dx /= self.batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-1. Root Mean Square Prop (RMSprop)\n",
    "\n",
    "- Geoffrey Hinton이 제안\n",
    "- average of squared gradients를 이용해서 업데이트!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop(object):\n",
    "    def __init__(self, layers, learning_rate = 1e-3):\n",
    "        self.beta, self.eps = 0.9, 1e-8\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear', 'tmp']:\n",
    "                layer.weight_gradient = np.zeros_like(layer.weight)\n",
    "                layer.db = np.zeros_like(layer.b)\n",
    "                \n",
    "    def backward(self, dout):\n",
    "        self.layers.reverse()    # 뒤에서 부터 backward 하기 위해 순서 바꿔주기\n",
    "        for layer in self.layers:\n",
    "            dout = layer.backward(dout)\n",
    "        self.layers.reverse()\n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear']:\n",
    "                layer.v_w = self.beta * layer.v_w + (1-self.beta) * layer.weight_gradient**2\n",
    "                layer.v_b = self.beta * layer.v_b + (1-self.beta) * layer.db**2\n",
    "                \n",
    "                layer.weight = layer.weight - (self.learning_rate / np.sqrt(layer.v_w + self.eps)) * layer.weight_gradient\n",
    "                layer.b = layer.b - (self.learning_rate / np.sqrt(layer.v_b + self.eps)) * layer.db\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-2. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- Gradient Descent 업데이트가 예전엔 모든 샘플에 대해서 한번에 다 구해서 진행.\n",
    "    - 해당 방법이 너무 느려...\n",
    "- 따라서 batch에 맞춰서 Gradient를 구하고 업데이트 진행\n",
    "    - 사실상 Gradient Descent라고 말하면 모든 사람이 batch에 따라 진행하므로 해당 방버이 Gradient Descent라고 생각해도 무방\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, layers, learning_rate = 1e-3, momentum=False):\n",
    "        self.beta, self.eps = 0.9, 1e-8\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear', 'tmp']:\n",
    "                layer.weight_gradient = np.zeros_like(layer.weight)\n",
    "                layer.db = np.zeros_like(layer.b)\n",
    "                \n",
    "    def backward(self, dout):\n",
    "        self.layers.reverse()    # 뒤에서 부터 backward 하기 위해 순서 바꿔주기\n",
    "        for layer in self.layers:\n",
    "            dout = layer.backward(dout)\n",
    "        self.layers.reverse()\n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear', 'tmp']:\n",
    "                \n",
    "                if self.momentum:\n",
    "                    # implementation for momentum but still need to be fixed\n",
    "                    layer.v_w = (self.beta * layer.v_w) + (1 - self.beta) * layer.weight_gradient\n",
    "                    layer.v_b = (self.beta * layer.v_b) +  (1 - self.beta) * layer.db\n",
    "                    layer.weight = layer.weight - (self.learning_rate * layer.v_w)\n",
    "                    layer.b = layer.b - (self.learning_rate * layer.v_b)\n",
    "                \n",
    "                else:\n",
    "                # classic SGD\n",
    "                    layer.weight = layer.weight - (self.learning_rate * layer.weight_gradient)\n",
    "                    layer.b = layer.b - (self.learning_rate * layer.db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-3. Adaptive moment estimation (Adam)\n",
    "\n",
    "- RMSprop 와 SGD 알고리즘을 합친 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, layers, learning_rate = 1e-3):\n",
    "        self.beta1, self.beta2, self.eps = 0.9, 0.999, 1e-7\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear', 'tmp']:\n",
    "                layer.weight_gradient = np.zeros_like(layer.weight)\n",
    "                layer.db = np.zeros_like(layer.b)\n",
    "                \n",
    "    def backward(self, dout):\n",
    "        self.layers.reverse()    # 뒤에서 부터 backward 하기 위해 순서 바꿔주기\n",
    "        for layer in self.layers:\n",
    "            dout = layer.backward(dout)\n",
    "        self.layers.reverse()\n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.name in ['linear', 'tmp']:\n",
    "                layer.m += (1-self.beta1) * (layer.weight_gradient - layer.m)\n",
    "                layer.v += (1-self.beta2) * (layer.weight_gradient**2 - layer.v)\n",
    "                layer.weight -= self.learning_rate * layer.m / (np.sqrt(layer.v) + self.eps)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        feature = x.copy()\n",
    "        for layer in self.layers:\n",
    "            feature = layer(feature)\n",
    "        \n",
    "        return feature    # >>> logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train, y_train\n",
    "x_test, y_test = x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSprop!\n",
    "\n",
    "- 75 epoch\n",
    "    - train: 95%\n",
    "    - test: 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th train acc: 31.72833333333333, loss: 11.00409250410843\n",
      "1th test acc: 51.980000000000004\n",
      "===============\n",
      "2th train acc: 59.56166666666667, loss: 6.517889186758365\n",
      "2th test acc: 66.92\n",
      "===============\n",
      "3th train acc: 69.395, loss: 4.932943104580797\n",
      "3th test acc: 73.48\n",
      "===============\n",
      "4th train acc: 74.42166666666667, loss: 4.12274015816596\n",
      "4th test acc: 77.02\n",
      "===============\n",
      "5th train acc: 77.31333333333333, loss: 3.656658556034081\n",
      "5th test acc: 79.28\n",
      "===============\n",
      "6th train acc: 79.49666666666667, loss: 3.304746798804825\n",
      "6th test acc: 81.01\n",
      "===============\n",
      "7th train acc: 81.08166666666666, loss: 3.0492749811521356\n",
      "7th test acc: 82.07\n",
      "===============\n",
      "8th train acc: 82.23333333333333, loss: 2.863648245086932\n",
      "8th test acc: 82.89\n",
      "===============\n",
      "9th train acc: 83.19166666666666, loss: 2.7091831608069152\n",
      "9th test acc: 83.67\n",
      "===============\n",
      "10th train acc: 84.045, loss: 2.571642077065404\n",
      "10th test acc: 84.36\n",
      "===============\n",
      "11th train acc: 84.70333333333333, loss: 2.4655312800382623\n",
      "11th test acc: 84.87\n",
      "===============\n",
      "12th train acc: 85.37, loss: 2.358077308365207\n",
      "12th test acc: 85.35000000000001\n",
      "===============\n",
      "13th train acc: 86.00833333333333, loss: 2.2551901304882565\n",
      "13th test acc: 85.74000000000001\n",
      "===============\n",
      "14th train acc: 86.49166666666666, loss: 2.1772860010252906\n",
      "14th test acc: 86.16\n",
      "===============\n",
      "15th train acc: 86.925, loss: 2.1074409194378045\n",
      "15th test acc: 86.47\n",
      "===============\n",
      "16th train acc: 87.35333333333334, loss: 2.0384017426378667\n",
      "16th test acc: 86.8\n",
      "===============\n",
      "17th train acc: 87.79, loss: 1.9680193911920152\n",
      "17th test acc: 86.97\n",
      "===============\n",
      "18th train acc: 88.145, loss: 1.910800151276113\n",
      "18th test acc: 87.18\n",
      "===============\n",
      "19th train acc: 88.50333333333333, loss: 1.853043641501846\n",
      "19th test acc: 87.37\n",
      "===============\n",
      "20th train acc: 88.82333333333334, loss: 1.8014657350987793\n",
      "20th test acc: 87.58\n",
      "===============\n",
      "21th train acc: 89.125, loss: 1.7528428129167215\n",
      "21th test acc: 87.7\n",
      "===============\n",
      "22th train acc: 89.37333333333333, loss: 1.7128162084685086\n",
      "22th test acc: 87.92\n",
      "===============\n",
      "23th train acc: 89.63166666666666, loss: 1.6711777944451995\n",
      "23th test acc: 88.05\n",
      "===============\n",
      "24th train acc: 89.90166666666667, loss: 1.6276589359176121\n",
      "24th test acc: 88.18\n",
      "===============\n",
      "25th train acc: 90.105, loss: 1.5948854745573302\n",
      "25th test acc: 88.36\n",
      "===============\n",
      "26th train acc: 90.335, loss: 1.5578138543301259\n",
      "26th test acc: 88.55\n",
      "===============\n",
      "27th train acc: 90.51, loss: 1.5296071867659489\n",
      "27th test acc: 88.71\n",
      "===============\n",
      "28th train acc: 90.64166666666667, loss: 1.5083850273605208\n",
      "28th test acc: 88.84\n",
      "===============\n",
      "29th train acc: 90.81166666666667, loss: 1.4809842645838915\n",
      "29th test acc: 88.85\n",
      "===============\n",
      "30th train acc: 90.99833333333333, loss: 1.4508971525154357\n",
      "30th test acc: 88.99000000000001\n",
      "===============\n",
      "31th train acc: 91.21166666666667, loss: 1.416511881580058\n",
      "31th test acc: 89.07000000000001\n",
      "===============\n",
      "32th train acc: 91.39166666666667, loss: 1.3874993092283332\n",
      "32th test acc: 89.14\n",
      "===============\n",
      "33th train acc: 91.53, loss: 1.3652026101061743\n",
      "33th test acc: 89.21\n",
      "===============\n",
      "34th train acc: 91.66, loss: 1.3442490856299283\n",
      "34th test acc: 89.24\n",
      "===============\n",
      "35th train acc: 91.81833333333333, loss: 1.3187287673575776\n",
      "35th test acc: 89.34\n",
      "===============\n",
      "36th train acc: 91.96, loss: 1.2958947642561869\n",
      "36th test acc: 89.4\n",
      "===============\n",
      "37th train acc: 92.09833333333334, loss: 1.2735980992548943\n",
      "37th test acc: 89.4\n",
      "===============\n",
      "38th train acc: 92.21166666666667, loss: 1.255330924070475\n",
      "38th test acc: 89.46\n",
      "===============\n",
      "39th train acc: 92.35, loss: 1.2330342249483162\n",
      "39th test acc: 89.53999999999999\n",
      "===============\n",
      "40th train acc: 92.48166666666665, loss: 1.2118120655428875\n",
      "40th test acc: 89.55\n",
      "===============\n",
      "41th train acc: 92.60833333333333, loss: 1.191395810925007\n",
      "41th test acc: 89.63\n",
      "===============\n",
      "42th train acc: 92.71000000000001, loss: 1.175009080244866\n",
      "42th test acc: 89.64\n",
      "===============\n",
      "43th train acc: 92.82833333333333, loss: 1.1559360002728987\n",
      "43th test acc: 89.79\n",
      "===============\n",
      "44th train acc: 92.96833333333333, loss: 1.1333706662215572\n",
      "44th test acc: 89.84\n",
      "===============\n",
      "45th train acc: 93.04833333333333, loss: 1.1204761896207904\n",
      "45th test acc: 89.97\n",
      "===============\n",
      "46th train acc: 93.14333333333333, loss: 1.1051640869202854\n",
      "46th test acc: 89.99000000000001\n",
      "===============\n",
      "47th train acc: 93.24, loss: 1.0895831727647871\n",
      "47th test acc: 90.01\n",
      "===============\n",
      "48th train acc: 93.33833333333334, loss: 1.0737337119430113\n",
      "48th test acc: 89.99000000000001\n",
      "===============\n",
      "49th train acc: 93.46833333333333, loss: 1.0527801874667653\n",
      "49th test acc: 90.07\n",
      "===============\n",
      "50th train acc: 93.54833333333333, loss: 1.039885710865999\n",
      "50th test acc: 90.07\n",
      "===============\n",
      "51th train acc: 93.60833333333333, loss: 1.0302148534154238\n",
      "51th test acc: 90.13\n",
      "===============\n",
      "52th train acc: 93.70666666666668, loss: 1.0143653925936482\n",
      "52th test acc: 90.14\n",
      "===============\n",
      "53th train acc: 93.82666666666667, loss: 0.9950236769545134\n",
      "53th test acc: 90.18\n",
      "===============\n",
      "54th train acc: 93.88166666666666, loss: 0.9861587502910187\n",
      "54th test acc: 90.27\n",
      "===============\n",
      "55th train acc: 93.955, loss: 0.974338788145435\n",
      "55th test acc: 90.25999999999999\n",
      "===============\n",
      "56th train acc: 94.11, loss: 0.9493557397314495\n",
      "56th test acc: 90.3\n",
      "===============\n",
      "57th train acc: 94.15166666666667, loss: 0.9426398665018836\n",
      "57th test acc: 90.3\n",
      "===============\n",
      "58th train acc: 94.22833333333334, loss: 0.9302826597594822\n",
      "58th test acc: 90.36999999999999\n",
      "===============\n",
      "59th train acc: 94.32166666666667, loss: 0.9152391037252545\n",
      "59th test acc: 90.39\n",
      "===============\n",
      "60th train acc: 94.34166666666667, loss: 0.9120154845750629\n",
      "60th test acc: 90.46\n",
      "===============\n",
      "61th train acc: 94.46666666666667, loss: 0.8918678648863648\n",
      "61th test acc: 90.5\n",
      "===============\n",
      "62th train acc: 94.52333333333334, loss: 0.8827342772941552\n",
      "62th test acc: 90.48\n",
      "===============\n",
      "63th train acc: 94.56166666666667, loss: 0.8764849340273947\n",
      "63th test acc: 90.57\n",
      "===============\n",
      "64th train acc: 94.62666666666667, loss: 0.8660789116848316\n",
      "64th test acc: 90.60000000000001\n",
      "===============\n",
      "65th train acc: 94.74833333333333, loss: 0.846468561854499\n",
      "65th test acc: 90.64999999999999\n",
      "===============\n",
      "66th train acc: 94.80166666666666, loss: 0.8378722441206545\n",
      "66th test acc: 90.68\n",
      "===============\n",
      "67th train acc: 94.83166666666666, loss: 0.833036815395367\n",
      "67th test acc: 90.71000000000001\n",
      "===============\n",
      "68th train acc: 94.90333333333332, loss: 0.8214855134405137\n",
      "68th test acc: 90.7\n",
      "===============\n",
      "69th train acc: 95.00333333333333, loss: 0.8053674176895554\n",
      "69th test acc: 90.74\n",
      "===============\n",
      "70th train acc: 95.00166666666667, loss: 0.8056360526187379\n",
      "70th test acc: 90.75\n",
      "===============\n",
      "71th train acc: 95.07833333333333, loss: 0.7932788458763365\n",
      "71th test acc: 90.77\n",
      "===============\n",
      "72th train acc: 95.19166666666666, loss: 0.7750116706919171\n",
      "72th test acc: 90.79\n",
      "===============\n",
      "73th train acc: 95.25833333333334, loss: 0.7642662735959376\n",
      "73th test acc: 90.75999999999999\n",
      "===============\n",
      "74th train acc: 95.28999999999999, loss: 0.7591622079814166\n",
      "74th test acc: 90.81\n",
      "===============\n",
      "75th train acc: 95.35833333333333, loss: 0.7481481777736533\n",
      "75th test acc: 90.86999999999999\n",
      "===============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-06d182ce88a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0midx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-dfa36da55641>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_b\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_w\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_b\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "batch_size=8\n",
    "iteration = int(len(x_train) / batch_size)\n",
    "test_iteration = int(len(x_test) / batch_size)\n",
    "\n",
    "model = Sequential([Linear(784, 512),\n",
    "          ReLU(), \n",
    "          Linear(512, 512),\n",
    "          ReLU(),\n",
    "          Linear(512, 256),\n",
    "          ReLU(),\n",
    "          Linear(256, 128),\n",
    "          ReLU(),\n",
    "          Linear(128, 10)\n",
    "         ])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = RMSprop(model.layers, learning_rate=0.00001)\n",
    "\n",
    "for epo in range(EPOCH):\n",
    "    train_acc = []\n",
    "    loss_list = []\n",
    "    idx = 0\n",
    "    \n",
    "    for each in range(iteration):\n",
    "        if each+1 == iteration:\n",
    "            x = x_train[idx:].reshape(len(x_train)-idx, -1)\n",
    "            target = y_train[idx:]\n",
    "        else:\n",
    "            x = x_train[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_train[idx: idx+batch_size]\n",
    "        output = model(x)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        predicted = output.argmax(axis=1)\n",
    "        train_acc.append((predicted == target).sum()/batch_size)\n",
    "\n",
    "        dout = criterion.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.backward(dout)\n",
    "        optimizer.step()\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "    print(f'{epo+1}th train acc: {np.mean(train_acc)*100}, loss: {np.mean(loss_list)}')\n",
    "    \n",
    "    idx = 0\n",
    "    test_acc = []\n",
    "    for each in range(test_iteration):\n",
    "        if each+1 == test_iteration:\n",
    "            x = x_test[idx:].reshape(len(x_test)-idx, -1)\n",
    "            target = y_test[idx:]\n",
    "        else:\n",
    "            x = x_test[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_test[idx: idx+batch_size]\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, target)\n",
    "        predicted = output.argmax(axis=1)\n",
    "        test_acc.append((predicted == target).sum()/batch_size)\n",
    "        \n",
    "        idx += batch_size\n",
    "    print(f'{epo+1}th test acc: {np.mean(test_acc)*100}')\n",
    "    print('='*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train[:8000], y_train[:8000]\n",
    "x_test, y_test = x_test[:2000], y_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th train acc: 59.925, loss: 6.459326772196549\n",
      "1th test acc: 67.85\n",
      "===============\n",
      "2th train acc: 78.4125, loss: 3.4794938202381314\n",
      "2th test acc: 71.3\n",
      "===============\n",
      "3th train acc: 83.28750000000001, loss: 2.6937366523789135\n",
      "3th test acc: 73.85000000000001\n",
      "===============\n",
      "4th train acc: 85.9375, loss: 2.2666071149785183\n",
      "4th test acc: 75.7\n",
      "===============\n",
      "5th train acc: 88.425, loss: 1.86566948317343\n",
      "5th test acc: 76.75\n",
      "===============\n",
      "6th train acc: 90.1625, loss: 1.585617569500529\n",
      "6th test acc: 77.7\n",
      "===============\n",
      "7th train acc: 91.1375, loss: 1.4284661359286857\n",
      "7th test acc: 77.64999999999999\n",
      "===============\n",
      "8th train acc: 92.0875, loss: 1.2753442262945816\n",
      "8th test acc: 77.60000000000001\n",
      "===============\n",
      "9th train acc: 93.325, loss: 1.0758827913764726\n",
      "9th test acc: 78.4\n",
      "===============\n",
      "10th train acc: 94.2625, loss: 0.9247756437112382\n",
      "10th test acc: 78.14999999999999\n",
      "===============\n",
      "11th train acc: 94.4875, loss: 0.8885099282715819\n",
      "11th test acc: 78.35\n",
      "===============\n",
      "12th train acc: 95.3, loss: 0.7575504002950456\n",
      "12th test acc: 78.7\n",
      "===============\n",
      "13th train acc: 95.9375, loss: 0.6547975398826863\n",
      "13th test acc: 78.60000000000001\n",
      "===============\n",
      "14th train acc: 96.0625, loss: 0.6346499201939886\n",
      "14th test acc: 78.85\n",
      "===============\n",
      "15th train acc: 96.55, loss: 0.5560742034080668\n",
      "15th test acc: 78.9\n",
      "===============\n",
      "16th train acc: 97.0625, loss: 0.47346896268440536\n",
      "16th test acc: 78.9\n",
      "===============\n",
      "17th train acc: 97.425, loss: 0.41504086558718156\n",
      "17th test acc: 78.8\n",
      "===============\n",
      "18th train acc: 97.5, loss: 0.4029522937739628\n",
      "18th test acc: 79.05\n",
      "===============\n",
      "19th train acc: 97.8625, loss: 0.34452419667673895\n",
      "19th test acc: 79.4\n",
      "===============\n",
      "20th train acc: 98.1875, loss: 0.2921403854861244\n",
      "20th test acc: 79.10000000000001\n",
      "===============\n",
      "21th train acc: 98.2625, loss: 0.2800518136729057\n",
      "21th test acc: 79.35\n",
      "===============\n",
      "22th train acc: 98.4, loss: 0.25788943201533804\n",
      "22th test acc: 79.5\n",
      "===============\n",
      "23th train acc: 98.5, loss: 0.24177133626437966\n",
      "23th test acc: 79.25\n",
      "===============\n",
      "24th train acc: 98.7125, loss: 0.20752038279359328\n",
      "24th test acc: 79.2\n",
      "===============\n",
      "25th train acc: 98.725, loss: 0.2055056208247235\n",
      "25th test acc: 79.05\n",
      "===============\n",
      "26th train acc: 98.9875, loss: 0.16319561947845793\n",
      "26th test acc: 79.2\n",
      "===============\n",
      "27th train acc: 99.125, loss: 0.14103323782089022\n",
      "27th test acc: 79.35\n",
      "===============\n",
      "28th train acc: 99.3375, loss: 0.10678228435010381\n",
      "28th test acc: 79.55\n",
      "===============\n",
      "29th train acc: 99.1875, loss: 0.13095942797654128\n",
      "29th test acc: 79.4\n",
      "===============\n",
      "30th train acc: 99.3125, loss: 0.11081180828784339\n",
      "30th test acc: 79.35\n",
      "===============\n",
      "31th train acc: 99.2625, loss: 0.11887085616332255\n",
      "31th test acc: 79.60000000000001\n",
      "===============\n",
      "32th train acc: 99.4, loss: 0.09670847450575484\n",
      "32th test acc: 79.45\n",
      "===============\n",
      "33th train acc: 99.3625, loss: 0.10275276041236422\n",
      "33th test acc: 79.45\n",
      "===============\n",
      "34th train acc: 99.47500000000001, loss: 0.08461990269253614\n",
      "34th test acc: 79.0\n",
      "===============\n",
      "35th train acc: 99.52499999999999, loss: 0.07656085481705696\n",
      "35th test acc: 79.2\n",
      "===============\n",
      "36th train acc: 99.425, loss: 0.09267895056801528\n",
      "36th test acc: 79.0\n",
      "===============\n",
      "37th train acc: 99.58749999999999, loss: 0.06648704497270802\n",
      "37th test acc: 79.25\n",
      "===============\n",
      "38th train acc: 99.55000000000001, loss: 0.07253133087931737\n",
      "38th test acc: 79.25\n",
      "===============\n",
      "39th train acc: 99.775, loss: 0.03626561543966116\n",
      "39th test acc: 79.5\n",
      "===============\n",
      "40th train acc: 99.7, loss: 0.0483541872528799\n",
      "40th test acc: 79.5\n",
      "===============\n",
      "41th train acc: 99.7625, loss: 0.03828037740853095\n",
      "41th test acc: 79.25\n",
      "===============\n",
      "42th train acc: 99.7125, loss: 0.04633942528401011\n",
      "42th test acc: 79.55\n",
      "===============\n",
      "43th train acc: 99.825, loss: 0.028206567564181998\n",
      "43th test acc: 79.65\n",
      "===============\n",
      "44th train acc: 99.8125, loss: 0.0302213295330518\n",
      "44th test acc: 79.4\n",
      "===============\n",
      "45th train acc: 99.8375, loss: 0.02619180559531221\n",
      "45th test acc: 79.9\n",
      "===============\n",
      "46th train acc: 99.8625, loss: 0.022162281657572634\n",
      "46th test acc: 80.05\n",
      "===============\n",
      "47th train acc: 99.8375, loss: 0.026191805595312215\n",
      "47th test acc: 79.95\n",
      "===============\n",
      "48th train acc: 99.88749999999999, loss: 0.01813275771983305\n",
      "48th test acc: 79.2\n",
      "===============\n",
      "49th train acc: 99.8125, loss: 0.030221329533051792\n",
      "49th test acc: 79.5\n",
      "===============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-fb29e9bd5338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-46-bf21c195176d>\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tmp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "EPOCH = 100\n",
    "batch_size=8\n",
    "iteration = int(len(x_train) / batch_size)\n",
    "test_iteration = int(len(x_test) / batch_size)\n",
    "\n",
    "model = Sequential([Linear(784, 512),\n",
    "          ReLU(), \n",
    "          Linear(512, 256),\n",
    "          ReLU(),\n",
    "          #Linear(256, 10),\n",
    "          #ReLU(),\n",
    "          Linear(256, 256),\n",
    "          ReLU(),\n",
    "          Linear(256, 128),\n",
    "          ReLU(),\n",
    "          #Linear(128, 64),\n",
    "          #ReLU(),\n",
    "          Linear(128, 10)\n",
    "         ])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.layers, learning_rate=0.00000001, momentum=False)\n",
    "\n",
    "for epo in range(EPOCH):\n",
    "    train_acc = []\n",
    "    loss_list = []\n",
    "    idx = 0\n",
    "    \n",
    "    for each in range(iteration):\n",
    "        if each+1 == iteration:\n",
    "            x = x_train[idx:].reshape(len(x_train)-idx, -1)\n",
    "            target = y_train[idx:]\n",
    "        else:\n",
    "            x = x_train[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_train[idx: idx+batch_size]\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        predicted = output.argmax(axis=1)\n",
    "        train_acc.append((predicted == target).sum()/batch_size)\n",
    "\n",
    "        dout = criterion.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.backward(dout)\n",
    "        optimizer.step()\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "    print(f'{epo+1}th train acc: {np.mean(train_acc)*100}, loss: {np.mean(loss_list)}')\n",
    "    \n",
    "    idx = 0\n",
    "    test_acc = []\n",
    "    for each in range(test_iteration):\n",
    "        if each+1 == test_iteration:\n",
    "            x = x_test[idx:].reshape(len(x_test)-idx, -1)\n",
    "            target = y_test[idx:]\n",
    "        else:\n",
    "            x = x_test[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_test[idx: idx+batch_size]\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, target)\n",
    "        predicted = output.argmax(axis=1)\n",
    "        test_acc.append((predicted == target).sum()/batch_size)\n",
    "        \n",
    "        idx += batch_size\n",
    "    print(f'{epo+1}th test acc: {np.mean(test_acc)*100}')\n",
    "    print('='*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = x_train[:8000], y_train[:8000]\n",
    "x_test, y_test = x_test[:2000], y_test[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th train acc: 12.35, loss: 14.127510825714968\n",
      "1th test acc: 15.9\n",
      "===============\n",
      "2th train acc: 19.400000000000002, loss: 12.991185075272405\n",
      "2th test acc: 19.3\n",
      "===============\n",
      "3th train acc: 23.825, loss: 12.2779593382925\n",
      "3th test acc: 22.45\n",
      "===============\n",
      "4th train acc: 28.6375, loss: 11.502275980277632\n",
      "4th test acc: 25.75\n",
      "===============\n",
      "5th train acc: 33.7375, loss: 10.680253096978758\n",
      "5th test acc: 29.549999999999997\n",
      "===============\n",
      "6th train acc: 38.5125, loss: 9.910614024870497\n",
      "6th test acc: 33.0\n",
      "===============\n",
      "7th train acc: 42.6125, loss: 9.24977209908121\n",
      "7th test acc: 36.1\n",
      "===============\n",
      "8th train acc: 46.5375, loss: 8.617136840856094\n",
      "8th test acc: 38.7\n",
      "===============\n",
      "9th train acc: 50.125, loss: 8.038900155790463\n",
      "9th test acc: 40.699999999999996\n",
      "===============\n",
      "10th train acc: 53.300000000000004, loss: 7.527150615697537\n",
      "10th test acc: 43.6\n",
      "===============\n",
      "11th train acc: 55.8875, loss: 7.110094888141491\n",
      "11th test acc: 46.25\n",
      "===============\n",
      "12th train acc: 58.099999999999994, loss: 6.753482019651538\n",
      "12th test acc: 48.6\n",
      "===============\n",
      "13th train acc: 59.95, loss: 6.45529724825881\n",
      "13th test acc: 50.64999999999999\n",
      "===============\n",
      "14th train acc: 62.3625, loss: 6.066448188266941\n",
      "14th test acc: 52.15\n",
      "===============\n",
      "15th train acc: 64.375, loss: 5.742071511278904\n",
      "15th test acc: 53.900000000000006\n",
      "===============\n",
      "16th train acc: 66.21249999999999, loss: 5.445901501855046\n",
      "16th test acc: 55.400000000000006\n",
      "===============\n",
      "17th train acc: 67.7375, loss: 5.200100541652931\n",
      "17th test acc: 56.15\n",
      "===============\n",
      "18th train acc: 68.8875, loss: 5.014742440516911\n",
      "18th test acc: 57.550000000000004\n",
      "===============\n",
      "19th train acc: 70.0875, loss: 4.821325291505411\n",
      "19th test acc: 58.85\n",
      "===============\n",
      "20th train acc: 71.2375, loss: 4.63596719036939\n",
      "20th test acc: 59.8\n",
      "===============\n",
      "21th train acc: 72.41250000000001, loss: 4.446579565295631\n",
      "21th test acc: 60.95\n",
      "===============\n",
      "22th train acc: 73.46249999999999, loss: 4.277339559910568\n",
      "22th test acc: 61.45\n",
      "===============\n",
      "23th train acc: 74.4, loss: 4.12623241224534\n",
      "23th test acc: 62.7\n",
      "===============\n",
      "24th train acc: 75.175, loss: 4.001317170175407\n",
      "24th test acc: 63.449999999999996\n",
      "===============\n",
      "25th train acc: 75.9375, loss: 3.8784166900743493\n",
      "25th test acc: 64.14999999999999\n",
      "===============\n",
      "26th train acc: 76.67500000000001, loss: 3.7595457339110325\n",
      "26th test acc: 64.4\n",
      "===============\n",
      "27th train acc: 77.4375, loss: 3.636645253809975\n",
      "27th test acc: 64.9\n",
      "===============\n",
      "28th train acc: 77.925, loss: 3.558069537024053\n",
      "28th test acc: 65.60000000000001\n",
      "===============\n",
      "29th train acc: 78.53750000000001, loss: 3.4593462005494335\n",
      "29th test acc: 66.3\n",
      "===============\n",
      "30th train acc: 79.05, loss: 3.3767409598257716\n",
      "30th test acc: 66.55\n",
      "===============\n",
      "31th train acc: 79.53750000000001, loss: 3.2981652430398505\n",
      "31th test acc: 67.2\n",
      "===============\n",
      "32th train acc: 80.0, loss: 3.223619050191668\n",
      "32th test acc: 67.35\n",
      "===============\n",
      "33th train acc: 80.6125, loss: 3.124895713717048\n",
      "33th test acc: 67.55\n",
      "===============\n",
      "34th train acc: 81.0125, loss: 3.060423330713215\n",
      "34th test acc: 68.10000000000001\n",
      "===============\n",
      "35th train acc: 81.45, loss: 2.989906661802773\n",
      "35th test acc: 68.45\n",
      "===============\n",
      "36th train acc: 81.875, loss: 2.9214047548611997\n",
      "36th test acc: 68.85\n",
      "===============\n",
      "37th train acc: 82.3, loss: 2.852902847919627\n",
      "37th test acc: 69.25\n",
      "===============\n",
      "38th train acc: 82.7625, loss: 2.778356655071445\n",
      "38th test acc: 69.55\n",
      "===============\n",
      "39th train acc: 83.15, loss: 2.715899034036481\n",
      "39th test acc: 70.05\n",
      "===============\n",
      "40th train acc: 83.475, loss: 2.6635152228458665\n",
      "40th test acc: 70.39999999999999\n",
      "===============\n",
      "41th train acc: 83.9875, loss: 2.5809099821222055\n",
      "41th test acc: 70.7\n",
      "===============\n",
      "42th train acc: 84.25, loss: 2.53859998077594\n",
      "42th test acc: 71.0\n",
      "===============\n",
      "43th train acc: 84.5875, loss: 2.4842014076164554\n",
      "43th test acc: 71.2\n",
      "===============\n",
      "44th train acc: 85.0875, loss: 2.4036109288616636\n",
      "44th test acc: 71.05\n",
      "===============\n",
      "45th train acc: 85.45, loss: 2.34518283176444\n",
      "45th test acc: 71.15\n",
      "===============\n",
      "46th train acc: 85.82499999999999, loss: 2.2847399726983464\n",
      "46th test acc: 71.5\n",
      "===============\n",
      "47th train acc: 86.1, loss: 2.240415209383211\n",
      "47th test acc: 71.6\n",
      "===============\n",
      "48th train acc: 86.425, loss: 2.1880313981925963\n",
      "48th test acc: 71.89999999999999\n",
      "===============\n",
      "49th train acc: 86.8625, loss: 2.1175147292821537\n",
      "49th test acc: 72.15\n",
      "===============\n",
      "50th train acc: 87.075, loss: 2.0832637758113686\n",
      "50th test acc: 72.3\n",
      "===============\n",
      "51th train acc: 87.5125, loss: 2.012747106900924\n",
      "51th test acc: 72.55\n",
      "===============\n",
      "52th train acc: 87.825, loss: 1.961686552275557\n",
      "52th test acc: 72.65\n",
      "===============\n",
      "53th train acc: 88.0625, loss: 1.924097580270654\n",
      "53th test acc: 72.55\n",
      "===============\n",
      "54th train acc: 88.4, loss: 1.8696990071111694\n",
      "54th test acc: 72.65\n",
      "===============\n",
      "55th train acc: 88.5, loss: 1.8535809113578687\n",
      "55th test acc: 73.0\n",
      "===============\n",
      "56th train acc: 88.8125, loss: 1.8032118621384663\n",
      "56th test acc: 73.05\n",
      "===============\n",
      "57th train acc: 88.9, loss: 1.7891085268798987\n",
      "57th test acc: 73.2\n",
      "===============\n",
      "58th train acc: 89.3, loss: 1.7246361453525447\n",
      "58th test acc: 73.35000000000001\n",
      "===============\n",
      "59th train acc: 89.4375, loss: 1.702473763694977\n",
      "59th test acc: 73.6\n",
      "===============\n",
      "60th train acc: 89.6875, loss: 1.662178524317581\n",
      "60th test acc: 73.65\n",
      "===============\n",
      "61th train acc: 89.8, loss: 1.6440456665977532\n",
      "61th test acc: 73.95\n",
      "===============\n",
      "62th train acc: 90.05, loss: 1.6037504272203573\n",
      "62th test acc: 74.05000000000001\n",
      "===============\n",
      "63th train acc: 90.08749999999999, loss: 1.597706141313748\n",
      "63th test acc: 74.05000000000001\n",
      "===============\n",
      "64th train acc: 90.4125, loss: 1.5453223301231334\n",
      "64th test acc: 74.15\n",
      "===============\n",
      "65th train acc: 90.6875, loss: 1.500997566807998\n",
      "65th test acc: 74.15\n",
      "===============\n",
      "66th train acc: 90.7875, loss: 1.4848794710570397\n",
      "66th test acc: 74.4\n",
      "===============\n",
      "67th train acc: 90.7875, loss: 1.4848794710570397\n",
      "67th test acc: 74.25\n",
      "===============\n",
      "68th train acc: 91.14999999999999, loss: 1.4264513739598157\n",
      "68th test acc: 74.65\n",
      "===============\n",
      "69th train acc: 91.3125, loss: 1.4002594683645087\n",
      "69th test acc: 74.75\n",
      "===============\n",
      "70th train acc: 91.55, loss: 1.361999249371797\n",
      "70th test acc: 74.8\n",
      "===============\n",
      "71th train acc: 92.0, loss: 1.2894475600766702\n",
      "71th test acc: 74.9\n",
      "===============\n",
      "72th train acc: 92.175, loss: 1.261240892512493\n",
      "72th test acc: 74.75\n",
      "===============\n",
      "73th train acc: 92.03750000000001, loss: 1.2834032741700607\n",
      "73th test acc: 75.05\n",
      "===============\n",
      "74th train acc: 92.35, loss: 1.233034224948316\n",
      "74th test acc: 75.05\n",
      "===============\n",
      "75th train acc: 92.5375, loss: 1.2028127954152692\n",
      "75th test acc: 75.14999999999999\n",
      "===============\n",
      "76th train acc: 92.63749999999999, loss: 1.1866946996643108\n",
      "76th test acc: 75.05\n",
      "===============\n",
      "77th train acc: 92.5875, loss: 1.1947537475397898\n",
      "77th test acc: 75.25\n",
      "===============\n",
      "78th train acc: 93.175, loss: 1.1000599350029097\n",
      "78th test acc: 75.1\n",
      "===============\n",
      "79th train acc: 93.075, loss: 1.1161780307538682\n",
      "79th test acc: 75.14999999999999\n",
      "===============\n",
      "80th train acc: 93.27499999999999, loss: 1.0839418392519515\n",
      "80th test acc: 75.3\n",
      "===============\n",
      "81th train acc: 93.55, loss: 1.039617075936816\n",
      "81th test acc: 75.35\n",
      "===============\n",
      "82th train acc: 93.8875, loss: 0.9852208633650592\n",
      "82th test acc: 75.44999999999999\n",
      "===============\n",
      "83th train acc: 93.7125, loss: 1.013425170341509\n",
      "83th test acc: 75.2\n",
      "===============\n",
      "84th train acc: 94.2375, loss: 0.9288051676489777\n",
      "84th test acc: 75.35\n",
      "===============\n",
      "85th train acc: 94.27499999999999, loss: 0.9227608817423684\n",
      "85th test acc: 75.25\n",
      "===============\n",
      "86th train acc: 94.525, loss: 0.8824656423649726\n",
      "86th test acc: 75.35\n",
      "===============\n",
      "87th train acc: 94.39999999999999, loss: 0.9026132620536705\n",
      "87th test acc: 75.4\n",
      "===============\n",
      "88th train acc: 94.6875, loss: 0.8562737367696653\n",
      "88th test acc: 75.4\n",
      "===============\n",
      "89th train acc: 94.6875, loss: 0.8562737367696653\n",
      "89th test acc: 75.4\n",
      "===============\n",
      "90th train acc: 95.0375, loss: 0.7998604016413112\n",
      "90th test acc: 75.4\n",
      "===============\n",
      "91th train acc: 95.0, loss: 0.8059046875479206\n",
      "91th test acc: 75.44999999999999\n",
      "===============\n",
      "92th train acc: 95.1375, loss: 0.7837423058903529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92th test acc: 75.3\n",
      "===============\n",
      "93th train acc: 95.3875, loss: 0.7434470665129571\n",
      "93th test acc: 75.44999999999999\n",
      "===============\n",
      "94th train acc: 95.3625, loss: 0.7474765904506967\n",
      "94th test acc: 75.44999999999999\n",
      "===============\n",
      "95th train acc: 95.55, loss: 0.7172551609176498\n",
      "95th test acc: 75.44999999999999\n",
      "===============\n",
      "96th train acc: 95.8875, loss: 0.6628565877581655\n",
      "96th test acc: 75.4\n",
      "===============\n",
      "97th train acc: 95.7, loss: 0.6930780172912124\n",
      "97th test acc: 75.4\n",
      "===============\n",
      "98th train acc: 96.0, loss: 0.6447237310819005\n",
      "98th test acc: 75.55\n",
      "===============\n",
      "99th train acc: 96.0875, loss: 0.6306203962562489\n",
      "99th test acc: 75.44999999999999\n",
      "===============\n",
      "100th train acc: 96.2125, loss: 0.6104727765675511\n",
      "100th test acc: 75.55\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "EPOCH = 100\n",
    "batch_size=8\n",
    "iteration = int(len(x_train) / batch_size)\n",
    "test_iteration = int(len(x_test) / batch_size)\n",
    "\n",
    "model = Sequential([Linear(784, 512),\n",
    "          ReLU(), \n",
    "          Linear(512, 256),\n",
    "          ReLU(),\n",
    "          #Linear(256, 10),\n",
    "          #ReLU(),\n",
    "          Linear(256, 256),\n",
    "          ReLU(),\n",
    "          Linear(256, 128),\n",
    "          ReLU(),\n",
    "          #Linear(128, 64),\n",
    "          #ReLU(),\n",
    "          Linear(128, 10)\n",
    "         ])\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.layers, learning_rate=0.00001)\n",
    "\n",
    "for epo in range(EPOCH):\n",
    "    train_acc = []\n",
    "    loss_list = []\n",
    "    idx = 0\n",
    "    \n",
    "    for each in range(iteration):\n",
    "        if each+1 == iteration:\n",
    "            x = x_train[idx:].reshape(len(x_train)-idx, -1)\n",
    "            target = y_train[idx:]\n",
    "        else:\n",
    "            x = x_train[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_train[idx: idx+batch_size]\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        predicted = output.argmax(axis=1)\n",
    "        train_acc.append((predicted == target).sum()/batch_size)\n",
    "\n",
    "        dout = criterion.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.backward(dout)\n",
    "        optimizer.step()\n",
    "        \n",
    "        idx += batch_size\n",
    "        \n",
    "    print(f'{epo+1}th train acc: {np.mean(train_acc)*100}, loss: {np.mean(loss_list)}')\n",
    "    \n",
    "    idx = 0\n",
    "    test_acc = []\n",
    "    for each in range(test_iteration):\n",
    "        if each+1 == test_iteration:\n",
    "            x = x_test[idx:].reshape(len(x_test)-idx, -1)\n",
    "            target = y_test[idx:]\n",
    "        else:\n",
    "            x = x_test[idx: idx+batch_size].reshape(batch_size, -1)\n",
    "            target = y_test[idx: idx+batch_size]\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, target)\n",
    "        predicted = output.argmax(axis=1)\n",
    "        test_acc.append((predicted == target).sum()/batch_size)\n",
    "        \n",
    "        idx += batch_size\n",
    "    print(f'{epo+1}th test acc: {np.mean(test_acc)*100}')\n",
    "    print('='*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
