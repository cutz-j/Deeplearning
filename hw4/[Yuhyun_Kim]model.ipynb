{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Yuhyun_Kim]model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeZy_t1Y52Xj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "0961ad9b-3d08-4693-9f0a-5bb546be2df6"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset Class\n",
        "class FashionMnist(Dataset):\n",
        "    def __init__(self, data_path, is_train=True):\n",
        "        filename = os.path.join(data_path, 'fashion-mnist_train.csv' if is_train else 'fashion-mnist_test.csv')\n",
        "        assert os.path.exists(filename), 'File not found error'\n",
        "        self.is_train = is_train\n",
        "        self.data = pd.read_csv(filename)\n",
        "        self.data = self.data.sort_values(by=['label']).to_numpy(dtype=np.float32)\n",
        "        self.data_y = self.data[:, 0].astype(np.int)\n",
        "        self.data_x = self.data[:, 1:]\n",
        "        self.data_shape = (28, 28)\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Grayscale(num_output_channels=1), # grayscale 변경\n",
        "                transforms.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(75), \n",
        "                transforms.ToTensor()\n",
        "            ]\n",
        "        )\n",
        "        self.transform_valid = transforms.Compose( ### test transform 추가\n",
        "            [\n",
        "                transforms.ToTensor()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ret_x = np.reshape(self.data_x[index], self.data_shape)\n",
        "        if self.is_train: # augmentation\n",
        "            ret_x = self.transform(ret_x)\n",
        "        else:\n",
        "            ret_x = self.transform_valid(ret_x) ### 기존 ret_x는 np, 이는 GPU에서 사용 못함\n",
        "                                                ### --> 텐서로 바꿔줌으로써 해결\n",
        "        ret_y = self.data_y[index]\n",
        "        return {\n",
        "            'data_x': ret_x,\n",
        "            'data_y': ret_y\n",
        "        }\n",
        "\n",
        "# Model Class\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # C -> M -> B -> R ==> C -> B -> R -> M\n",
        "        )\n",
        "  \n",
        "    def forward(self, tensor):\n",
        "        return self.block(tensor)\n",
        "\n",
        "    \n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, in_channel, num_classes):\n",
        "        super(BasicModel, self).__init__()\n",
        "        k = 32\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(in_channel, k),\n",
        "            ConvBlock(k, k*2),\n",
        "            ConvBlock(k*2, k*4),\n",
        "            ConvBlock(k*4, k*2)\n",
        "        )\n",
        "        self.linear = nn.Sequential( # Linear --> Sequential\n",
        "            nn.Linear(k*2, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, tensor):\n",
        "        out = self.block(tensor)\n",
        "        out = out.view(-1, out.size(1))\n",
        "        return self.linear(out)\n",
        "\n",
        "def main():\n",
        "    batch_size = 100 ### 3 -> 100\n",
        "    epoch = 30\n",
        "    learning_rate = 1e-03\n",
        "    in_channel = 1 # grayscale --> not 3\n",
        "    num_classes = 10\n",
        "    betas = (0.5, 0.999)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "    train_dataset = FashionMnist('/content/drive/My Drive', True)\n",
        "    valid_dataset = FashionMnist('/content/drive/My Drive', False)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True, drop_last=False, pin_memory=True, num_workers=2)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle = True, drop_last=False, pin_memory=True, num_workers=2)\n",
        "                                                                ### shuffle 추가\n",
        "\n",
        "    model = BasicModel(in_channel=in_channel, num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()# nn.BCELoss() # Binary Cross Entropy Loss ### 클래스가 여러개일 때 BCLE 사용 불가 (텐서 크기 안맞는 오류)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas)\n",
        "    for ep in range(epoch):\n",
        "        # train\n",
        "        avg_loss = 0\n",
        "        avg_acc = 0\n",
        "\n",
        "        count = 0\n",
        "        for idx, batch in enumerate(train_loader): ### enum\n",
        "\n",
        "            optim.zero_grad()\n",
        "            batch_x = batch['data_x'].to(device)\n",
        "            batch_y = batch['data_y'].to(device) ### .to(device) 추가\n",
        "            ### to(device)를 안해줘서 cpu인지 gpu인지 모호해짐\n",
        "            \n",
        "            output = model.forward(batch_x) ### forward 추가\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            # cal accuracy\n",
        "            _, index = torch.max(output, 1)\n",
        "            avg_acc += (index == batch_y).sum().float() / len(batch_y)\n",
        "            count += 1\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "        avg_loss /= count\n",
        "        avg_acc /= count\n",
        "\n",
        "        # print(\"half success\")\n",
        "\n",
        "        # valid\n",
        "        avg_test_loss = 0\n",
        "        avg_test_acc = 0\n",
        "\n",
        "        count = 0\n",
        "        with torch.no_grad():    \n",
        "            for idx, batch in enumerate(valid_loader): ### enum 수정\n",
        "                # optim.zero_grad()\n",
        "                batch_x = batch['data_x'].to(device) ### to.device 추가\n",
        "                batch_y = batch['data_y'].to(device)\n",
        "                output = model.forward(batch_x) ### forward 추가\n",
        "\n",
        "                loss = criterion(output, batch_y)\n",
        "                avg_test_loss += loss.item()\n",
        "\n",
        "                # cal accuracy\n",
        "                _, index = torch.max(output, 1)\n",
        "                avg_test_acc += (index == batch_y).sum().float() / len(batch_y)\n",
        "                count += 1\n",
        "\n",
        "        avg_test_loss /= count\n",
        "        avg_test_acc /= count\n",
        "\n",
        "        print(\"[Epoch:%03d] train loss: %.5f train accuracy: %.4f | valid loss: %.5f valid accuracy: %.4f\"\n",
        "              % (ep+1, avg_loss, avg_acc, avg_test_loss, avg_test_acc))\n",
        "\n",
        "    print(\"Training Done.\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Epoch:001] train loss: 0.90221 train accuracy: 0.6750 | valid loss: 0.59051 valid accuracy: 0.7737\n",
            "[Epoch:002] train loss: 0.65934 train accuracy: 0.7584 | valid loss: 0.49380 valid accuracy: 0.8250\n",
            "[Epoch:003] train loss: 0.59178 train accuracy: 0.7819 | valid loss: 0.46040 valid accuracy: 0.8336\n",
            "[Epoch:004] train loss: 0.54460 train accuracy: 0.8015 | valid loss: 0.42770 valid accuracy: 0.8428\n",
            "[Epoch:005] train loss: 0.52184 train accuracy: 0.8097 | valid loss: 0.42683 valid accuracy: 0.8464\n",
            "[Epoch:006] train loss: 0.49800 train accuracy: 0.8192 | valid loss: 0.44268 valid accuracy: 0.8324\n",
            "[Epoch:007] train loss: 0.48793 train accuracy: 0.8226 | valid loss: 0.39488 valid accuracy: 0.8549\n",
            "[Epoch:008] train loss: 0.46822 train accuracy: 0.8292 | valid loss: 0.36329 valid accuracy: 0.8689\n",
            "[Epoch:009] train loss: 0.46060 train accuracy: 0.8337 | valid loss: 0.38830 valid accuracy: 0.8565\n",
            "[Epoch:010] train loss: 0.44896 train accuracy: 0.8361 | valid loss: 0.37562 valid accuracy: 0.8634\n",
            "[Epoch:011] train loss: 0.43952 train accuracy: 0.8408 | valid loss: 0.34764 valid accuracy: 0.8719\n",
            "[Epoch:012] train loss: 0.43243 train accuracy: 0.8420 | valid loss: 0.35937 valid accuracy: 0.8674\n",
            "[Epoch:013] train loss: 0.42362 train accuracy: 0.8458 | valid loss: 0.36376 valid accuracy: 0.8679\n",
            "[Epoch:014] train loss: 0.42138 train accuracy: 0.8459 | valid loss: 0.34365 valid accuracy: 0.8748\n",
            "[Epoch:015] train loss: 0.40968 train accuracy: 0.8504 | valid loss: 0.32997 valid accuracy: 0.8803\n",
            "[Epoch:016] train loss: 0.40651 train accuracy: 0.8520 | valid loss: 0.33280 valid accuracy: 0.8794\n",
            "[Epoch:017] train loss: 0.40429 train accuracy: 0.8516 | valid loss: 0.33470 valid accuracy: 0.8791\n",
            "[Epoch:018] train loss: 0.39891 train accuracy: 0.8537 | valid loss: 0.33514 valid accuracy: 0.8771\n",
            "[Epoch:019] train loss: 0.39442 train accuracy: 0.8558 | valid loss: 0.34240 valid accuracy: 0.8750\n",
            "[Epoch:020] train loss: 0.38956 train accuracy: 0.8580 | valid loss: 0.32144 valid accuracy: 0.8805\n",
            "[Epoch:021] train loss: 0.38421 train accuracy: 0.8601 | valid loss: 0.34321 valid accuracy: 0.8733\n",
            "[Epoch:022] train loss: 0.38248 train accuracy: 0.8614 | valid loss: 0.32927 valid accuracy: 0.8790\n",
            "[Epoch:023] train loss: 0.37923 train accuracy: 0.8613 | valid loss: 0.31535 valid accuracy: 0.8831\n",
            "[Epoch:024] train loss: 0.37751 train accuracy: 0.8627 | valid loss: 0.30728 valid accuracy: 0.8836\n",
            "[Epoch:025] train loss: 0.37567 train accuracy: 0.8628 | valid loss: 0.31614 valid accuracy: 0.8850\n",
            "[Epoch:026] train loss: 0.37171 train accuracy: 0.8639 | valid loss: 0.32842 valid accuracy: 0.8815\n",
            "[Epoch:027] train loss: 0.36818 train accuracy: 0.8656 | valid loss: 0.32645 valid accuracy: 0.8779\n",
            "[Epoch:028] train loss: 0.36463 train accuracy: 0.8677 | valid loss: 0.31083 valid accuracy: 0.8840\n",
            "[Epoch:029] train loss: 0.36278 train accuracy: 0.8659 | valid loss: 0.32573 valid accuracy: 0.8784\n",
            "[Epoch:030] train loss: 0.35841 train accuracy: 0.8689 | valid loss: 0.31548 valid accuracy: 0.8866\n",
            "Training Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}