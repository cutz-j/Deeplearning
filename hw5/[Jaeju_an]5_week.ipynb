{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 과제의 목표인 아키텍처를 다루는데 집중해 봤습니다.\n",
    "    - RNNs 구현을 잘 못해서 연습으로 붙여봤습니다.\n",
    "    \n",
    "    \n",
    "- 데이터셋 중 택시, 버스, 영업용은 label의 한글로 나누었습니다.\n",
    "    - 앞에 지역명이 붙은 label = 영업용 차량\n",
    "    - [아/바/사/자]를 포함한 차량 번호 = 택시/버스\n",
    "    \n",
    "    \n",
    "- 초록색 번호판은 패턴매칭을 통해서 제외시켰습니다.\n",
    "    - blur + adaptive thresholding 후, 신규 번호판의 패턴에 맞는 번호판만 골라내었습니다.\n",
    "    - 결과, [02버1520]을 제외하고 초록판을 모두 제외하였습니다.\n",
    "    - 그러나, 신규 번호판도 10-20개 가량 제외되었습니다.\n",
    "    - 개선안으로 초록색 detection 후, blur + adaptive thresholding을 적용할 예정입니다.\n",
    "    \n",
    "    \n",
    "- 데이터셋 구성 후, Train과 Test로 나눈다음 loader를 구성했습니다.\n",
    "\n",
    "\n",
    "- 모델은 CNN2RNN 아키텍처입니다.\n",
    "    - 예정: test process 구현\n",
    "    - 한계: local에서 작업해서 ... cpu가 열일을 했으나... 힘이 부족했습니다... \n",
    "    - google colab으로 작업 시, opencv가 한글을 인식못해서 전처리를 다시 해야해서 수정계획입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#from torchvision.datasets import MNIST >>> 나중에 mnist pretrained 시도용\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader#, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contour(w, h, area, ratio):\n",
    "    MIN_AREA = 100\n",
    "    MIN_WIDTH, MIN_HEIGHT = 5, 25\n",
    "    MIN_RATIO, MAX_RATIO = 0.2, 0.5\n",
    "    return area > MIN_AREA and w > MIN_WIDTH and h > MIN_HEIGHT and MIN_RATIO < ratio < MAX_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_green(img):\n",
    "    height, width, channel = img.shape\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 가우시안 블러 + AdaptiveThresholding\n",
    "    img_blurred = cv2.GaussianBlur(gray, ksize=(3, 3), sigmaX=0)\n",
    "    img_blur_thresh = cv2.adaptiveThreshold(gray, maxValue=255.0,\n",
    "                                           adaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                           thresholdType=cv2.THRESH_BINARY_INV,\n",
    "                                           blockSize=19,\n",
    "                                           C=9)\n",
    "\n",
    "    # 윤곽찾기 -> Pattern Matching\n",
    "    # 가로 신축형 윤곽이 아니면 초록색(옛날 번호판)으로 구분\n",
    "    contours, _ = cv2.findContours(img_blur_thresh,\n",
    "                                  mode=cv2.RETR_LIST,\n",
    "                                  method=cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_dict = []\n",
    "    cnt = 0\n",
    "\n",
    "    #idx = 0\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = w * h\n",
    "        ratio = w / h\n",
    "        cx = x + (w / 2),\n",
    "        cy = y + (h / 2)\n",
    "\n",
    "        if check_contour(w, h, area, ratio):\n",
    "            cnt += 1\n",
    "            #name = 'conout' + str(idx)\n",
    "            contours_dict.append({\n",
    "            'contour': contour,\n",
    "            'x1': x,\n",
    "            'y1': y,\n",
    "            'x2': x+w,\n",
    "            'y2': y+h,\n",
    "            'w': w,\n",
    "            'h': h,\n",
    "            'cx': cx,\n",
    "            'cy': cy\n",
    "            })\n",
    "\n",
    "    if len(contours_dict) == 0:\n",
    "        return True\n",
    "    \n",
    "    # 신형 번호판의 최소 만족 갯수 3개\n",
    "    if cnt < 3:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    # bbox 중복제거용.\n",
    "    # test1. 동작 X contours_dict를 우선순위 heap으로 만들고 while문으로 구현\n",
    "    # test2. 초록색 탐지 -> 초록색 아닌걸로 판명난 걸 이 method로 탐지\n",
    "    \"\"\"idx = []\n",
    "    for i in range(len(contours_dict)):\n",
    "        box1 = contours_dict[i]\n",
    "\n",
    "        for j in range(i+1, len(contours_dict)):\n",
    "            box2 = contours_dict[j]\n",
    "\n",
    "            x_overlap  = max(0, min(box1['x2'], box2['x2']) - max(box1['x1'], box2['x1']))\n",
    "            y_overlap  = max(0, min(box1['y2'], box2['y2']) - max(box1['y1'], box2['y1']))\n",
    "            overallaped_area = x_overlap * y_overlap\n",
    "\n",
    "            if overallaped_area > 3:\n",
    "                if box1['w']*box1['h'] < box2['w']*box2['h']:\n",
    "                    del contours_dict[i]\n",
    "                else:\n",
    "                    del contours_dict[j]\n",
    "\n",
    "    for i in range(len(idx)):\n",
    "        cnt -= 1\n",
    "        del contours_dict[idx[i][0]]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 제외함수 (택시, 버스, 영업용, )\n",
    "def exclude_data(file_list, path):\n",
    "    tmp = [name for name in file_list if not any(re.findall(r'바|사|아|자', name))] # taxi 제외\n",
    "    res = [name for name in tmp if not any(re.findall('[가-힣]+', name[0:2]))]  # 앞에 지역명을 갖고있는 영업용차량 제외\n",
    "\n",
    "    valid_data = []\n",
    "    for name in tqdm(res):\n",
    "\n",
    "        img_pil = Image.open(path+name)\n",
    "        img_np = np.array(img_pil)\n",
    "        img = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if not is_green(img):\n",
    "            valid_data.append(name)\n",
    "    return valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 981/981 [00:01<00:00, 773.90it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    path = 'C:/Users/anjae/Desktop/PerspectiveImages/'\n",
    "    file_list = os.listdir(path)\n",
    "    valid_data = exclude_data(file_list, path)    # >>> 812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [name[:-4] for name in valid_data] # eliminate the .jpg\n",
    "counter = Counter()\n",
    "for cap in captions:\n",
    "    for token in cap:\n",
    "        counter.update(token)\n",
    "        \n",
    "words = [word for word, cnt in counter.items()]\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<start>',\n",
       " 1: '<end>',\n",
       " 2: '0',\n",
       " 3: '1',\n",
       " 4: '도',\n",
       " 5: '9',\n",
       " 6: '8',\n",
       " 7: '2',\n",
       " 8: '두',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '3',\n",
       " 12: '오',\n",
       " 13: '호',\n",
       " 14: '4',\n",
       " 15: '거',\n",
       " 16: '모',\n",
       " 17: '버',\n",
       " 18: '5',\n",
       " 19: '주',\n",
       " 20: '고',\n",
       " 21: '나',\n",
       " 22: '라',\n",
       " 23: '로',\n",
       " 24: '루',\n",
       " 25: '보',\n",
       " 26: '부',\n",
       " 27: '가',\n",
       " 28: '마',\n",
       " 29: '머',\n",
       " 30: '소',\n",
       " 31: '조',\n",
       " 32: '하',\n",
       " 33: '구',\n",
       " 34: '누',\n",
       " 35: '러',\n",
       " 36: '노',\n",
       " 37: '수',\n",
       " 38: '우',\n",
       " 39: '무',\n",
       " 40: '저',\n",
       " 41: '더',\n",
       " 42: '너',\n",
       " 43: '어',\n",
       " 44: '다',\n",
       " 45: '서'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 0,\n",
       " '<end>': 1,\n",
       " '0': 2,\n",
       " '1': 3,\n",
       " '도': 4,\n",
       " '9': 5,\n",
       " '8': 6,\n",
       " '2': 7,\n",
       " '두': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '3': 11,\n",
       " '오': 12,\n",
       " '호': 13,\n",
       " '4': 14,\n",
       " '거': 15,\n",
       " '모': 16,\n",
       " '버': 17,\n",
       " '5': 18,\n",
       " '주': 19,\n",
       " '고': 20,\n",
       " '나': 21,\n",
       " '라': 22,\n",
       " '로': 23,\n",
       " '루': 24,\n",
       " '보': 25,\n",
       " '부': 26,\n",
       " '가': 27,\n",
       " '마': 28,\n",
       " '머': 29,\n",
       " '소': 30,\n",
       " '조': 31,\n",
       " '하': 32,\n",
       " '구': 33,\n",
       " '누': 34,\n",
       " '러': 35,\n",
       " '노': 36,\n",
       " '수': 37,\n",
       " '우': 38,\n",
       " '무': 39,\n",
       " '저': 40,\n",
       " '더': 41,\n",
       " '너': 42,\n",
       " '어': 43,\n",
       " '다': 44,\n",
       " '서': 45}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2idx"
   ]
  },
  {
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fInd mean and std for each channel for valid_data\n",
    "imgs = []\n",
    "for file_name in valid_data:\n",
    "    img = Image.open(path + file_name)\n",
    "    imgs.append(np.asarray(img))\n",
    "\n",
    "imgs = np.asarray(imgs)\n",
    "normalize_mean = [imgs[:,:,:,0].mean(), imgs[:,:,:,1].mean(), imgs[:,:,:,2].mean()]\n",
    "normalize_std = [imgs[:,:,:,0].std(), imgs[:,:,:,1].std(), imgs[:,:,:,2].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([140.35294402260286, 141.85065142150805, 140.9446750838778],\n",
       " [67.1339938663095, 66.77541666677854, 66.45039705287245])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mean, normalize_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalize_mean,normalize_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, imgs, vocab, captions, transform):\n",
    "        self.imgs = imgs\n",
    "        self.captions = captions\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        imgs = self.imgs\n",
    "        vocab = self.vocab\n",
    "        captions = self.captions\n",
    "        \n",
    "        image = imgs[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        tokens = [token for token in captions[index]]\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "shuffle = False\n",
    "\n",
    "c = CustomDataset(imgs, vocab, captions, transform)\n",
    "data_loder = DataLoader(dataset=c,\n",
    "                       batch_size=batch_size,\n",
    "                       shuffle=shuffle,\n",
    "                       )\n",
    "\n",
    "train_size = int(0.8 * c.__len__())\n",
    "test_size = c.__len__() - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "train_dataset, test_dataset = data.random_split(c, [train_size, test_size])\n",
    "train_loder = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loder = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mnist데이터 훈련된 vgg11 모델이 필요하다\n",
    "# 나중에 업데이트\n",
    "class MnistVgg11():\n",
    "    def __init__(self):\n",
    "        self.vgg11 = models.vgg11(pretrained=True).features\n",
    "        self.train_batch_size = 16\n",
    "        train_loader = DataLoader(MNIST(download=True, root=\".\", transform=data_transform, train=True),\n",
    "                              batch_size=train_batch_size, shuffle=True)\n",
    "        output = self.vgg1(train_loader.sampler.data_source)\n",
    "        self.vgg11.classifier = nn.Linear(output, 10)\n",
    "    \n",
    "    def train(self):\n",
    "        epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image captionning approach\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(input_dim, 32, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.BatchNorm2d(32),\n",
    "                                  nn.Conv2d(32, 64, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.BatchNorm2d(64),\n",
    "                                  nn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.BatchNorm2d(128),\n",
    "                                  nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.AdaptiveAvgPool2d((7,7))\n",
    "                                 )\n",
    "        self.linear = nn.Linear(6272, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.conv(images)\n",
    "  \n",
    "        return self.dropout(self.relu(self.linear(features.reshape(images.shape[0], -1))))\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1, batch_first=True):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)  # 46 x 256\n",
    "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, \\\n",
    "                            num_layers=num_layers, batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, feature, caption):\n",
    "        \"\"\"CNN으로 얻은 feature 벡터를 복호화하고 캡션을 생성\"\"\"\n",
    "        \n",
    "        # batch x seq_length x embed_size\n",
    "        # 8 x 8 x 256\n",
    "        x = self.embed(caption) \n",
    "        \n",
    "        # batch x num_layers*num_directions x embed_size\n",
    "        # 8 x 1 x 256\n",
    "        h0 = feature\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        \n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        output = self.linear(out)\n",
    "        return output\n",
    "\n",
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, hidden_size, vocab_size, num_layers=1, batch_first=True):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(input_dim, embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers, batch_first=batch_first)\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def forward(self, x, caption):\n",
    "        feature = self.encoder(x)    # 8 x 256\n",
    "        feature_ = feature.unsqueeze(0)    # 8 x 1 x 256\n",
    "\n",
    "        caption_ = caption[:,:-1]    # 8 x 9 -> 8 x 8\n",
    "        output = self.decoder(feature_, caption_)\n",
    "        return output\n",
    "    \n",
    "    def sample_caption(self, image, vocab):\n",
    "        result = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if len(image.shape) < 4:\n",
    "                image = image.unsqueeze(0)\n",
    "            feature = self.encoder(image).unsqueeze(0) # image를 한개씩 받기 때문에 batch차원 늘려준다\n",
    "            states = None\n",
    "            hidden = feature\n",
    "            \n",
    "            for _ in range(8):\n",
    "                hiddens, states = self.decoder.lstm(hidden, states)\n",
    "                output = self.decoder.linear(hiddens).squeeze(0)\n",
    "                predicted = output.argmax(1)\n",
    "                result.append(predicted.item())\n",
    "                x = self.decoder.embed(predicted).unsqueeze(0)\n",
    "                hidden = hiddens\n",
    "                \n",
    "                if vocab.idx2word[predicted.item()] == '<end>':\n",
    "                    break\n",
    "                    \n",
    "        return [vocab.idx2word[i] for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, epo, filename='my_checkpoint'):\n",
    "    print(\">>> saving checkpoing\")\n",
    "    torch.save(state, filename + str(epo) + '.path.tar')\n",
    "    \n",
    "def load_checkpoint(checkpoint):\n",
    "    model.load_state_dict(torch.load(checkpoint)['state_dict'])\n",
    "    optimizer.load_state_dict(torch.load(checkpoint)['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.003\n",
    "epoch = 100\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(vocab)\n",
    "input_dim = train_dataset.dataset.imgs.shape[-1]\n",
    "model = CNN2RNN(input_dim, embed_size, hidden_size, len(vocab))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epo = 3    # custom number for what epoch model and optimizer \n",
    "checkpoint = 'my_checkpoint3.path.tar'\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:42,  6.44s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 148.75156569480896\n",
      ">>> saving checkpoing\n",
      "['4', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:30,  6.30s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 141.91908645629883\n",
      ">>> saving checkpoing\n",
      "['루', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:29,  6.30s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss: 135.66598272323608\n",
      ">>> saving checkpoing\n",
      "['보', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:35,  6.37s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 130.0629551410675\n",
      ">>> saving checkpoing\n",
      "['보', '6', '9', '0', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:20,  6.18s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss: 122.58750653266907\n",
      ">>> saving checkpoing\n",
      "['보', '9', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:25,  5.50s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 115.40042090415955\n",
      ">>> saving checkpoing\n",
      "['보', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:06,  5.26s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 108.32889890670776\n",
      ">>> saving checkpoing\n",
      "['0', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:10,  5.32s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, loss: 102.56735575199127\n",
      ">>> saving checkpoing\n",
      "['7', '4', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [10:54,  8.08s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, loss: 98.12836933135986\n",
      ">>> saving checkpoing\n",
      "['7', '7', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [06:34,  4.87s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, loss: 94.72585034370422\n",
      ">>> saving checkpoing\n",
      "['버', '0', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:37,  5.65s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, loss: 91.15415805578232\n",
      ">>> saving checkpoing\n",
      "['보', '4', '4', '4', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:27,  6.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, loss: 88.22106629610062\n",
      ">>> saving checkpoing\n",
      "['노', '4', '4', '7', '<end>']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:36,  6.38s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, loss: 85.68246454000473\n",
      ">>> saving checkpoing\n",
      "['보', '4', '7', '조', '0', '0', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [11:05,  8.22s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, loss: 84.04882782697678\n",
      ">>> saving checkpoing\n",
      "['버', '4', '4', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:49,  5.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, loss: 82.98073101043701\n",
      ">>> saving checkpoing\n",
      "['9', '0', '4', '8', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [10:28,  7.75s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, loss: 82.05763804912567\n",
      ">>> saving checkpoing\n",
      "['9', '5', '5', '0', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:25,  6.24s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 81.9950961470604\n",
      ">>> saving checkpoing\n",
      "['9', '도', '도', '4', '1', '1', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:14,  5.36s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, loss: 80.62081855535507\n",
      ">>> saving checkpoing\n",
      "['9', '무', '도', '4', '1', '1', '1', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:13,  6.83s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, loss: 80.41172277927399\n",
      ">>> saving checkpoing\n",
      "['9', '호', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [11:11,  8.29s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, loss: 79.40023404359818\n",
      ">>> saving checkpoing\n",
      "['주', '호', '도', '6', '1', '7', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:07,  6.76s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, loss: 79.83566373586655\n",
      ">>> saving checkpoing\n",
      "['버', '라', '노', '다', '나', '나', '나', '나']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:01,  6.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, loss: 80.06698137521744\n",
      ">>> saving checkpoing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['버', '0', '1', '1', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:41,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, loss: 78.82182770967484\n",
      ">>> saving checkpoing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['주', '주', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:44,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, loss: 78.58829414844513\n",
      ">>> saving checkpoing\n",
      "['호', '0', '0', '4', '<end>']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:12,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, loss: 78.43253004550934\n",
      ">>> saving checkpoing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [07:48,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, loss: 78.36071783304214\n",
      ">>> saving checkpoing\n",
      "['9', '0', '<end>']"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:58,  7.39s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, loss: 78.59089982509613\n",
      ">>> saving checkpoing\n",
      "['버', '오', '나', '나', '나', '나', '조', '조']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:49,  6.54s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, loss: 77.58725446462631\n",
      ">>> saving checkpoing\n",
      "['주', '오', '너', '너', '나', '9', '7', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:45,  6.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, loss: 77.48627364635468\n",
      ">>> saving checkpoing\n",
      "['주', '조', '마', '오', '오', '오', '오', '오']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:32,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, loss: 77.74176013469696\n",
      ">>> saving checkpoing\n",
      "['호', '조', '나', '오', '다', '다', '다', '다']\n",
      "<start>06모"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [10:42,  7.94s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, loss: 77.56473100185394\n",
      ">>> saving checkpoing\n",
      "['버', '주', '4', '4', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [11:57,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35, loss: 76.98919636011124\n",
      ">>> saving checkpoing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['너', '주', '4', '버', '8', '8', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:34,  7.10s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36, loss: 77.3264462351799\n",
      ">>> saving checkpoing\n",
      "['너', '1', '4', '8', '8', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [08:06,  6.00s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37, loss: 76.82195454835892\n",
      ">>> saving checkpoing\n",
      "['노', '라', '나', '나', '8', '오', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [09:32,  7.07s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38, loss: 76.85344737768173\n",
      ">>> saving checkpoing\n",
      "['너', '라', '나', '8', '8', '8', '<end>']\n",
      "<start>06모1297<end>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [08:26, 17.04s/it]"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(checkpoint)\n",
    "\n",
    "for img, caption in test_loder:\n",
    "    sample_img = img[0]\n",
    "    sample_caption = caption[0]\n",
    "    break\n",
    "    \n",
    "for epo in range(4, epoch):\n",
    "    running_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for idx, (image, caption) in tqdm(enumerate(train_loder)):\n",
    "        image = image.to(device)\n",
    "        caption = caption.type(torch.LongTensor).to(device)\n",
    "\n",
    "        output = model(image, caption)   \n",
    "        result = output.reshape(-1, output.shape[2])\n",
    "        target = caption[:, 1:].reshape(-1)\n",
    "        loss = criterion(result, target)\n",
    "    \n",
    "        predicted = [each.argmax(1) for each in output]\n",
    "        predicted_ = torch.stack(predicted)\n",
    "        correct = caption[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'epoch: {epo}, loss: {running_loss}')\n",
    "    checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, epo)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = model.sample_caption(sample_img, vocab)\n",
    "        print(res)\n",
    "        for each in sample_caption:\n",
    "            print(vocab.idx2word[each.item()], end='')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과가 처음에 2번째 step만에 끝내버리던 것이 step을 점점 더 잘 보고 키우고있습니다.\n",
    "\n",
    "- sample test결과가 안좋은 이유에 대해서는 2가지 개선 예정 입니다.\n",
    "    - CNN을 pretrained 시켜서 적용해보는 방법\n",
    "    - CNN의 구조를 fancy하게 바꾸는 방법\n",
    "\n",
    "- 30 epoch만에 2 step-> 6 step으로 왔는데 개선이 필요해한 것 같습니다.\n",
    "\n",
    "- 오늘 study중 구현 방법에서 CNN 으로만 구현, feature map의 channel을 seq_length에 맞춰서 예측해보는 방법도 적용해 볼 예정입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
