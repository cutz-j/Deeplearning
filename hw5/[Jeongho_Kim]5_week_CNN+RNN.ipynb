{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.nn import Conv2d, BatchNorm2d, MaxPool2d, ReLU, Sequential, Linear, Embedding, GRU\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torch.utils.data import Dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_FEATURE = 256\n",
    "NUM_HIDDEN = 50\n",
    "LENGTH = 7\n",
    "NUM_CHARACTER = 44\n",
    "NUM_EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hangeul_to_idx = {\"가\":\"a\", \"거\":\"b\", \"고\":\"c\", \"구\":\"d\", \"나\":\"e\", \"너\":\"f\", \"노\":\"g\", \"누\":\"h\", \"다\":\"i\", \"더\":\"j\", \"도\":\"k\", \"두\":\"l\", \"라\":\"m\", \"러\":\"n\", \"로\":\"o\", \n",
    "                  \"루\":\"p\", \"마\":\"q\", \"머\": \"r\",\"모\":\"s\", \"무\":\"t\", \"버\":\"u\", \"보\":\"v\", \"부\":\"w\", \"서\":\"x\", \"소\":\"y\", \"수\":\"z\", \"어\":\"A\", \"오\":\"B\", \"우\":\"C\", \"저\":\"D\", \n",
    "                  \"조\":\"E\", \"주\":\"F\", \"하\":\"G\", \"호\":\"H\"}\n",
    "idx_to_char = \"0123456789가거고구나너노누다더도두라러로루마머모무버보부서소수어오우저조주하호\"\n",
    "\n",
    "#파일명에 한글이 들어가서 사진을 읽을 때 이 함수로 읽는다 \n",
    "def my_cv_read_binary(filepath):\n",
    "    ff = np.fromfile(filepath, dtype = np.uint8)\n",
    "    gray_img = cv2.imdecode(ff, cv2.IMREAD_GRAYSCALE)\n",
    "    (thresh, im_bw) = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return im_bw\n",
    "\n",
    "def my_cv_read(filepath):\n",
    "    ff = np.fromfile(filepath, dtype = np.float)\n",
    "    img = cv2.imdecode(ff, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "#신식 번호판인지 구식 번호판인지 분류하는 함수\n",
    "def version_classifier(image):\n",
    "    if image[10,10] ==0:\n",
    "        #옛날 번호판\n",
    "        return 0\n",
    "    else:\n",
    "        #신식 번호판\n",
    "        return 1\n",
    "\n",
    "#output을 번호판 문자열로\n",
    "def output_to_plate(output:torch.Tensor):\n",
    "    output = output.reshape(BATCH_SIZE, LENGTH,-1).cpu().detach().numpy()\n",
    "    result = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        temp = []\n",
    "        for j in range(LENGTH):\n",
    "            temp.append(idx_to_char[output[i][j].argmax()])\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(torch.nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size = 3, stride = 1, padding = 1, pool_width = 2):\n",
    "        super(conv_block, self).__init__()\n",
    "        model = []\n",
    "        model.append(Conv2d(in_channel, out_channel, kernel_size = kernel_size, stride = stride, padding = padding))\n",
    "        model.append(BatchNorm2d(out_channel))\n",
    "        model.append(ReLU())\n",
    "        model.append(MaxPool2d((pool_width, pool_width)))\n",
    "        self.model = Sequential(*model)\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        return self.model(input_)\n",
    "    \n",
    "class CNN_RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size,  hidden_size,output_size, batch_size, num_layers = 2):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        self.cnn_block1 = conv_block(3, 16)\n",
    "        self.cnn_block2 = conv_block(16,32)\n",
    "        self.cnn_block3 = conv_block(32,64)\n",
    "        self.cnn_block4 = conv_block(64,128)\n",
    "        self.cnn_block5 = conv_block(128,256)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        #self.encoder = Embedding(input_size, embedding_size)\n",
    "        self.rnn = GRU(input_size, hidden_size, num_layers=2,bias=True, batch_first=True)\n",
    "        self.decoder = Linear(hidden_size, output_size)\n",
    "        \n",
    "    def cnn_features(self, input_):\n",
    "        x = self.cnn_block1(input_)\n",
    "        x = self.cnn_block2(x)\n",
    "        x = self.cnn_block3(x)\n",
    "        x = self.cnn_block4(x)\n",
    "        return self.cnn_block5(x)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "                             \n",
    "    def forward(self, input_, hidden):\n",
    "        x = self.cnn_features(input_)\n",
    "        x = x.reshape(self.batch_size, 7, 256)\n",
    "        #이 때 x의 shape는 (batch_size, 7,256)\n",
    "        \n",
    "        \n",
    "        # RNN 시작\n",
    "        #x = x.reshape(self.batch_size, -1)\n",
    "        #x = self.encoder(x)\n",
    "        x,hidden = self.rnn(x, hidden)\n",
    "        return self.decoder(x), hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, batch_size = BATCH_SIZE, n_character = NUM_CHARACTER, train = True, transform = None):\n",
    "        super(Custom_Dataset, self).__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.n_character = n_character\n",
    "        self.map = {\"0\" : 0, \"1\": 1, \"2\":2, \"3\":3, \"4\":4, \"5\":5, \"6\":6,\"7\":7,\"8\":8,\"9\":9,\"가\":10, \"거\":11, \"고\":12, \"구\":13, \n",
    "                    \"나\":14, \"너\":15, \"노\":16, \"누\":17, \"다\":18, \"더\":19, \"도\":20, \"두\":21, \"라\":22, \"러\":23, \"로\":24, \n",
    "                  \"루\":25, \"마\":26, \"머\": 27,\"모\":28, \"무\":29, \"버\":30, \"보\":31, \"부\":32, \"서\":33, \"소\":34, \"수\":35,\n",
    "                    \"어\":36, \"오\":37, \"우\":38, \"저\":39, \"조\":40, \"주\":41, \"하\":42, \"호\":43}\n",
    "\n",
    "        if train:\n",
    "            self.image_paths = glob.glob(\"./new_data\" + \"/train/\" + \"*\")\n",
    "        else:\n",
    "            self.image_paths = glob.glob(\"./new_data\" + \"/test/\"  +  \"*\")\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.image_paths[idx]\n",
    "        image = my_cv_read(file_path)\n",
    "        image = transform(image)\n",
    "        label = self.make_label(file_path[-11:-4])\n",
    "        return (image, label)\n",
    "    \n",
    "    def make_label(self, label):\n",
    "        tensor_label = torch.zeros(len(label), dtype = torch.long)\n",
    "        for i in range(len(label)):\n",
    "            tensor_label[i] = self.map[label[i]]\n",
    "        return tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToPILImage(), torchvision.transforms.Resize((32,228)), torchvision.transforms.ToTensor()])\n",
    "train_dataset = Custom_Dataset(\"./new_data\", train = True, transform = transform)\n",
    "test_dataset = Custom_Dataset(\"./new_data\", train = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, drop_last =True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN_RNN(NUM_FEATURE, NUM_HIDDEN, NUM_CHARACTER ,BATCH_SIZE).to(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss : 7.62939453125e-05\n",
      "epoch : 10, loss : 1.4850071238470264e-05\n",
      "epoch : 20, loss : 7.901872777438257e-06\n",
      "epoch : 30, loss : 3.9509363887191284e-06\n",
      "epoch : 40, loss : 1.8392290712654358e-06\n",
      "epoch : 50, loss : 8.174351364687027e-07\n",
      "epoch : 60, loss : 2.0435878411717567e-07\n",
      "epoch : 70, loss : 2.384185791015625e-07\n",
      "epoch : 80, loss : 0.13341949880123138\n",
      "epoch : 90, loss : 0.00021311214368324727\n",
      "epoch : 100, loss : 7.234300574054942e-05\n",
      "epoch : 110, loss : 2.3944037820911035e-05\n",
      "epoch : 120, loss : 7.595334864163306e-06\n",
      "epoch : 130, loss : 3.4740992305160034e-06\n",
      "epoch : 140, loss : 2.009528088819934e-06\n",
      "epoch : 150, loss : 1.1920928955078125e-06\n",
      "epoch : 160, loss : 6.130763381406723e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-608ddb3983fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\rlawjdghek\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\rlawjdghek\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.reshape(-1).to(device)\n",
    "        hidden = model.init_hidden().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output,hidden = model(image,hidden)\n",
    "        output = output.reshape(-1, NUM_CHARACTER)\n",
    "        loss = loss_func(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10==0:\n",
    "        print(\"epoch : {}, loss : {}\".format(epoch, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model/RNN+CNN.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 91.07142857142857%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 280\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for image, label in test_loader:\n",
    "        image = image.to(device)\n",
    "        hidden = model.init_hidden().to(device)\n",
    "        output2,_ = model(image,hidden)\n",
    "        np_image = image.cpu().detach().numpy().transpose(0,2,3,1)\n",
    "        predict = output_to_plate(output2)\n",
    "        for i in range(4):\n",
    "            for j in range(7):\n",
    "                if predict[i][j] == idx_to_char[label[i][j]]:\n",
    "                    correct +=1\n",
    "        \n",
    "    print(\"accuracy : {}%\".format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlawjdghek",
   "language": "python",
   "name": "rlawjdghek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
