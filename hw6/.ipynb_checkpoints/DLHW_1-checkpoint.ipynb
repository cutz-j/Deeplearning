{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "FNyOV80YL8T0",
    "outputId": "16205b1f-35e8-4906-d2d2-0af8be8dad7d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from    tensorflow.keras import datasets, layers, optimizers, models\n",
    "from    tensorflow.keras import regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YWPfkbZFNGYE"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.applications.vgg16.VGG16(weights=None,input_shape=(32,32,3),classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "9SdDFPKkNIqJ",
    "outputId": "150f8885-5dbe-4141-b759-ad47271b34b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import  os\n",
    "import  numpy as np\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "\n",
    "def normalize(X_train, X_test):\n",
    "    # this function normalize inputs for zero mean and unit variance\n",
    "    # it is used when training a model.\n",
    "    # Input: training set and test set\n",
    "    # Output: normalized training set and test set according to the trianing set statistics.\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    print('mean:', mean, 'std:', std)\n",
    "    X_train = (X_train - mean) / (std + 1e-7)\n",
    "    X_test = (X_test - mean) / (std + 1e-7)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_cifar(x, y):\n",
    "\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "tf.random.set_seed(22)\n",
    "\n",
    "print('loading data...')\n",
    "(x,y), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "# x = tf.convert_to_tensor(x)\n",
    "# y = tf.convert_to_tensor(y)\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "train_loader = train_loader.map(prepare_cifar).shuffle(50000).batch(256)\n",
    "\n",
    "test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_loader = test_loader.map(prepare_cifar).shuffle(10000).batch(256)\n",
    "print('done.')\n",
    "# must specify from_logits=True!\n",
    "criteon = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# model = VGG16([32, 32, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "Ou_L2YPNMDT6",
    "outputId": "a6063cae-106a-41b3-f36c-2c041c276ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 27.671894073486328 acc: 0.05859375\n",
      "0 80 loss: 23.923625946044922 acc: 0.27075195\n",
      "0 160 loss: 21.292884826660156 acc: 0.41020507\n",
      "test acc: 0.4682\n",
      "1 0 loss: 21.969139099121094 acc: 0.41015625\n",
      "1 80 loss: 19.95907974243164 acc: 0.49057618\n",
      "1 160 loss: 18.742429733276367 acc: 0.52958983\n",
      "test acc: 0.5567\n",
      "2 0 loss: 17.193065643310547 acc: 0.58984375\n",
      "2 80 loss: 17.104938507080078 acc: 0.5852051\n",
      "2 160 loss: 16.014442443847656 acc: 0.61152345\n",
      "test acc: 0.5578\n",
      "3 0 loss: 16.177104949951172 acc: 0.62109375\n",
      "3 80 loss: 14.964887619018555 acc: 0.65048826\n",
      "3 160 loss: 15.024389266967773 acc: 0.660498\n",
      "test acc: 0.5714\n",
      "4 0 loss: 15.086990356445312 acc: 0.66015625\n",
      "4 80 loss: 13.693796157836914 acc: 0.6942383\n",
      "4 160 loss: 13.082687377929688 acc: 0.6958496\n",
      "test acc: 0.6616\n",
      "5 0 loss: 12.787555694580078 acc: 0.75\n",
      "5 80 loss: 13.133464813232422 acc: 0.74838865\n",
      "5 160 loss: 14.494966506958008 acc: 0.7393066\n",
      "test acc: 0.6696\n",
      "6 0 loss: 12.851448059082031 acc: 0.7578125\n",
      "6 80 loss: 12.985835075378418 acc: 0.77978516\n",
      "6 160 loss: 11.740015983581543 acc: 0.7784668\n",
      "test acc: 0.6999\n",
      "7 0 loss: 9.908414840698242 acc: 0.82421875\n",
      "7 80 loss: 10.448662757873535 acc: 0.81645507\n",
      "7 160 loss: 11.154706954956055 acc: 0.81206053\n",
      "test acc: 0.6982\n",
      "8 0 loss: 11.337032318115234 acc: 0.80859375\n",
      "8 80 loss: 8.908143043518066 acc: 0.84819335\n",
      "8 160 loss: 9.555882453918457 acc: 0.8408203\n",
      "test acc: 0.7195\n",
      "9 0 loss: 7.51707649230957 acc: 0.88671875\n",
      "9 80 loss: 7.761812210083008 acc: 0.8660644\n",
      "9 160 loss: 8.579937934875488 acc: 0.8618652\n",
      "test acc: 0.6952\n",
      "10 0 loss: 10.153091430664062 acc: 0.8125\n",
      "10 80 loss: 6.754430770874023 acc: 0.8895019\n",
      "10 160 loss: 8.965683937072754 acc: 0.8821289\n",
      "test acc: 0.7106\n",
      "11 0 loss: 6.8675665855407715 acc: 0.8984375\n",
      "11 80 loss: 7.625095844268799 acc: 0.9097168\n",
      "11 160 loss: 7.097070217132568 acc: 0.8989258\n",
      "test acc: 0.7268\n",
      "12 0 loss: 5.847353935241699 acc: 0.9375\n",
      "12 80 loss: 7.104386329650879 acc: 0.91381836\n",
      "12 160 loss: 7.4506731033325195 acc: 0.9048828\n",
      "test acc: 0.6832\n",
      "13 0 loss: 8.345454216003418 acc: 0.875\n",
      "13 80 loss: 6.548442840576172 acc: 0.9218262\n",
      "13 160 loss: 6.504266262054443 acc: 0.9187012\n",
      "test acc: 0.7217\n",
      "14 0 loss: 6.359539031982422 acc: 0.92578125\n",
      "14 80 loss: 5.95781946182251 acc: 0.9307617\n",
      "14 160 loss: 5.765703201293945 acc: 0.92871094\n",
      "test acc: 0.706\n",
      "15 0 loss: 5.005476474761963 acc: 0.95703125\n",
      "15 80 loss: 6.438084602355957 acc: 0.9380859\n",
      "15 160 loss: 6.136802673339844 acc: 0.9393066\n",
      "test acc: 0.7127\n",
      "16 0 loss: 6.563244819641113 acc: 0.91015625\n",
      "16 80 loss: 5.455496788024902 acc: 0.9405762\n",
      "16 160 loss: 5.994939804077148 acc: 0.9367676\n",
      "test acc: 0.7284\n",
      "17 0 loss: 4.4443440437316895 acc: 0.96875\n",
      "17 80 loss: 4.715911388397217 acc: 0.9525879\n",
      "17 160 loss: 5.599709987640381 acc: 0.94091797\n",
      "test acc: 0.7271\n",
      "18 0 loss: 4.09011173248291 acc: 0.96875\n",
      "18 80 loss: 4.780506610870361 acc: 0.95166016\n",
      "18 160 loss: 5.188345909118652 acc: 0.94785154\n",
      "test acc: 0.7167\n",
      "19 0 loss: 5.957024574279785 acc: 0.93359375\n",
      "19 80 loss: 4.770510673522949 acc: 0.9524902\n",
      "19 160 loss: 4.652031898498535 acc: 0.946582\n",
      "test acc: 0.7267\n",
      "20 0 loss: 4.78286600112915 acc: 0.9609375\n",
      "20 80 loss: 4.663665294647217 acc: 0.95947266\n",
      "20 160 loss: 5.271897315979004 acc: 0.9511719\n",
      "test acc: 0.7191\n",
      "21 0 loss: 4.06347131729126 acc: 0.96875\n",
      "21 80 loss: 4.425031661987305 acc: 0.95874023\n",
      "21 160 loss: 4.439365386962891 acc: 0.9566406\n",
      "test acc: 0.7314\n",
      "22 0 loss: 3.981083869934082 acc: 0.9765625\n",
      "22 80 loss: 4.6730875968933105 acc: 0.96225584\n",
      "22 160 loss: 4.5804243087768555 acc: 0.96069336\n",
      "test acc: 0.7306\n",
      "23 0 loss: 4.828062057495117 acc: 0.953125\n",
      "23 80 loss: 4.0008440017700195 acc: 0.96489257\n",
      "23 160 loss: 4.590184211730957 acc: 0.9585937\n",
      "test acc: 0.7259\n",
      "24 0 loss: 4.271982192993164 acc: 0.96875\n",
      "24 80 loss: 4.756527900695801 acc: 0.96259767\n",
      "24 160 loss: 4.167396068572998 acc: 0.9559082\n",
      "test acc: 0.7289\n",
      "25 0 loss: 3.901981830596924 acc: 0.98046875\n",
      "25 80 loss: 4.465694427490234 acc: 0.96542966\n",
      "25 160 loss: 5.017821788787842 acc: 0.96308595\n",
      "test acc: 0.719\n",
      "26 0 loss: 4.936906814575195 acc: 0.953125\n",
      "26 80 loss: 5.363952159881592 acc: 0.9634277\n",
      "26 160 loss: 4.933340072631836 acc: 0.95585936\n",
      "test acc: 0.7394\n",
      "27 0 loss: 4.473874568939209 acc: 0.96484375\n",
      "27 80 loss: 4.5626373291015625 acc: 0.9675293\n",
      "27 160 loss: 3.9794912338256836 acc: 0.9652344\n",
      "test acc: 0.7253\n",
      "28 0 loss: 4.816019535064697 acc: 0.953125\n",
      "28 80 loss: 4.174685478210449 acc: 0.9692383\n",
      "28 160 loss: 4.9097185134887695 acc: 0.9641113\n",
      "test acc: 0.7251\n",
      "29 0 loss: 4.3909478187561035 acc: 0.96484375\n",
      "29 80 loss: 4.116333484649658 acc: 0.9687988\n",
      "29 160 loss: 4.453281402587891 acc: 0.96655273\n",
      "test acc: 0.7317\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        # [b, 1] => [b]\n",
    "        y = tf.squeeze(y, axis=1)\n",
    "        # [b, 10]\n",
    "        y = tf.one_hot(y, depth=10)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss = criteon(y, logits)\n",
    "            loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n",
    "            mse_loss = tf.reduce_sum(tf.square(y-logits))\n",
    "            # print(y.shape, logits.shape)\n",
    "            metric.update_state(y, logits)\n",
    "            loss = loss + loss2 + mse_loss*0.1\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 80 == 0:\n",
    "            # for g in grads:\n",
    "            #     print(tf.norm(g).numpy())\n",
    "            print(epoch, step, 'loss:', float(loss), 'acc:', metric.result().numpy())\n",
    "            metric.reset_states()\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        for x, y in test_loader:\n",
    "            # [b, 1] => [b]\n",
    "            y = tf.squeeze(y, axis=1)\n",
    "            # [b, 10]\n",
    "            y = tf.one_hot(y, depth=10)\n",
    "\n",
    "            logits = model.predict(x)\n",
    "            # be careful, these functions can accept y as [b] without warnning.\n",
    "            metric.update_state(y, logits)\n",
    "        print('test acc:', metric.result().numpy())\n",
    "        metric.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 주어진 VGG16모델의 첫번째 블록   \n",
    "## 두번째 convolution layer의 값을 output으로 가지는 model을 선언하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. 주어진 CIFAR10 test 데이터셋에서 testset 중 airplane class가 있는 폴더를 glob으로    \n",
    "## 불러와 주소를 리스트로 저장하고 그중 하나를 matplotlib을 통해 이미지를 보이시오 (RGB형태로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 다음 순서를 지켜 코딩하시오   \n",
    "### 3.1 optimizer(adam) , MSE loss를 선언한다\n",
    "### 3.2 새로운 이미지를 생성하기 위해 학습 가능한 variable을 생성한다    \n",
    "### 3.3 우리가 중간값만 내보내도록 새롭게 정의한 모델을 F(x)라고 정의하고 \n",
    "### 우리가 공격하고자 하는 원본 이미지를 A   \n",
    "### 우리가 이미지를 생성하기 위해 생성한 학습 가능한 variable을 B라고 가정할때    \n",
    "### 최적화의 목적식이 F(A) = F(B)가 되도록 F(A)와 F(B)의 오차를 선언하고, 학습하여 결과값을 print 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 결과값을 tf.keras.preprocessing.image.array_to_img와 matplotlib을 활용하여 이미지화 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 생성한 결과값을 **matplotlib 그래프 격자 무늬 없이** 원본 이미지만 jpg나 png 형태로 저장한 후  \n",
    "### 전체 1000개의 이미지에 대해 output을 계산한 뒤\n",
    "### 원본 이미지와의 psnr , ssim metric을 계산한다\n",
    "### 비행기 test data 1000개 이미지에서 전부 output을 계산한 뒤 psnr, ssim 결과의 평균을 제시하시오\n",
    "### **tf 패키지 내의 ssim , psnr metric을 활용할 것**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DLHW_1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
